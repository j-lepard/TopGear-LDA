{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Automatic Speech Recognition, Diarize and Label\n",
    "\n",
    "Environment = \"whisperx\"\n",
    "\n",
    "* Performance Benchmarks on local\n",
    "* GPU Benchmark: 0.09961056709289551 seconds\n",
    "* Memory Bandwidth Benchmark: 0.2920224666595459 seconds\n",
    "* CPU Benchmark: 13.046526432037354 seconds\n",
    "* Disk Write Benchmark: 2.3364615440368652 seconds\n",
    "* Disk Read Benchmark: 0.05882525444030762 seconds \\n\n",
    "  \n",
    "** all benchmarks are >> faster than Collab with the exception of Disk write.\n",
    "\n",
    "## Setup ⚙️\n",
    "Tested for PyTorch 2.0, Python 3.10 (use other versions at your own risk!)\n",
    "GPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the CTranslate2 documentation.\n",
    "\n",
    "1.  Create Python3.10 environment\n",
    "\n",
    "`conda create --name whisperx python=3.10`\n",
    "\n",
    "`conda activate whisperx`\n",
    "\n",
    "2. Install PyTorch, e.g. for Linux and Windows CUDA11.8:\n",
    "   \n",
    "conda install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "See other methods here.\n",
    "\n",
    "1. Install this repo\n",
    "\n",
    "`pip install git+https://github.com/m-bain/whisperx.git`\n",
    "\n",
    "If already installed, update package to most recent commit\n",
    "\n",
    "`pip install git+https://github.com/m-bain/whisperx.git --upgrade`\n",
    "\n",
    "## Post Setup - REQUIRED for DIARIZATION **Actually dont do this!!\n",
    "https://github.com/m-bain/whisperX/issues/499\n",
    "\n",
    "`pip install pyannote.audio==3.0.1`\n",
    "\n",
    "`pip uninstall onnxruntime`\n",
    "\n",
    "`pip install --force-reinstall onnxruntime-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess initial audio file\n",
    "convert to Wav using ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "## 1 - Convert Mp3 to WAV.\n",
    "\n",
    "def convert_m4a_to_mp3(input_file, output_file):\n",
    "    try:\n",
    "        ffmpeg.input(input_file).output(output_file).run(overwrite_output=True)\n",
    "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Input/ output files and usage\n",
    "input_mp3 = './audio/Botswana_2007_Audio.mp3'  # Change this to your mp3 file path\n",
    "output_wav = './data/Botswana_2007_Audio.wav'  # Change this to your desired output wav file path\n",
    "\n",
    "convert_m4a_to_mp3(input_mp3, output_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "## Full file should be the input (2007 or 2024 file..)\n",
    "audio_file = \"./data/Botswana_2007_Audio.wav\"\n",
    "\n",
    "## DEBUGGING, use a small file\n",
    "# audio_file = \"./data/Intro.wav\"\n",
    "\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "without_timestamps= 'True'\n",
    "\n",
    "## Some error handling to ensure that successfully loaded the mp3 file!\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"The file '{audio_file}' does not exist.\")\n",
    "    # Optionally, you can add more checks (like file format) here\n",
    "\n",
    "    print(f\"Successfully accessed the audio file: {audio_file}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "## Full file should be the input (2007 or 2024 file..)\n",
    "audio_file = \"./data/Botswana_2007_Audio.wav\"\n",
    "\n",
    "## DEBUGGING, use a small file\n",
    "# audio_file = \"./data/Intro.wav\"\n",
    "\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "without_timestamps= 'True'\n",
    "\n",
    "## Some error handling to ensure that successfully loaded the mp3 file!\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"The file '{audio_file}' does not exist.\")\n",
    "    # Optionally, you can add more checks (like file format) here\n",
    "\n",
    "    print(f\"Successfully accessed the audio file: {audio_file}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batches\n",
    "\n",
    "### Split the Audio file into smaller pieces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 - Split up large files in <10min\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import math\n",
    "\n",
    "\n",
    "# TODO: change max duration to 300 seconds\n",
    "# TODO: update the target database folder\n",
    "# TODO: check input filenames\n",
    "# TODO: Update the output folder\n",
    "\n",
    "# Function to split audio and save to database\n",
    "def split_audio(audio_file, max_duration=300):  # 60second (1min) for testing; 300sec for production\n",
    "    conn = sqlite3.connect('./data/Audio_clips.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS clips\n",
    "                     (id INTEGER PRIMARY KEY AUTOINCREMENT, start_time REAL, end_time REAL, filename TEXT)''')\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        return []\n",
    "\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    num_splits = math.ceil(total_duration / max_duration)\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        start_time = i * max_duration\n",
    "        end_time = min((i + 1) * max_duration, total_duration)\n",
    "\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        clip = y[start_sample:end_sample]\n",
    "        filename = f\"./data/Botswana2007_clip_{i}.wav\"\n",
    "\n",
    "        try:\n",
    "            sf.write(filename, clip, sr)\n",
    "            cursor.execute(\"INSERT INTO clips (start_time, end_time, filename) VALUES (?, ?, ?)\",\n",
    "                           (start_time, end_time, filename))\n",
    "            conn.commit()\n",
    "            results.append({\"start_time\": start_time, \"end_time\": end_time, \"filename\": filename})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing clip {i}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "    return results\n",
    "# results is a DIctionary\n",
    "results = split_audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH PROCESS: Transcript - Align - Diarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - use python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "from HF_token import TOKEN_ID\n",
    "\n",
    "# Directory containing .wav files\n",
    "wav_directory = './data/Testing'\n",
    "\n",
    "# Get a list of all .wav files in the directory\n",
    "wav_files = glob.glob(os.path.join(wav_directory, '*.wav'))\n",
    "\n",
    "# Initialize results_full list\n",
    "aligned_results_full = []\n",
    "\n",
    "# Set device and compute type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"float32\"\n",
    "\n",
    "print(f\"TRANSCRIBING & ALIGNING using device: {device}\")\n",
    "print(f\"Compute type is {compute_type}\")\n",
    "\n",
    "# Ensure the model directory exists\n",
    "model_dir = \"./model/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# # Load the model and save it to the local path\n",
    "# try:\n",
    "#     model = whisperx.load_model(\"large-v2\", device=device, compute_type=compute_type, download_root=model_dir)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     raise\n",
    "\n",
    "# Iterate through each .wav file and process it\n",
    "for wav_file in wav_files:\n",
    "    print(f\"Processing file: {wav_file}\")\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio = whisperx.load_audio(wav_file)\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        output_dir = \"./outputs/Testing\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        #####################     TRANSCRIPTION  #################\n",
    "        # Load the model and save it to the local path\n",
    "        try:\n",
    "            model = whisperx.load_model(\"large-v2\", device=device, compute_type=compute_type, download_root=model_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "        print(f\"STARTING Transcription on {wav_file}\")\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        transcribe_result = model.transcribe(audio, batch_size=batch_size)\n",
    "        print(transcribe_result[\"segments\"])  # before alignment\n",
    "        \n",
    "        # Save the transcription result to a JSON file\n",
    "        transcript_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{transcript_filename}_transcript.json', 'w') as json_file:\n",
    "            json.dump(transcribe_result, json_file, indent=4)\n",
    "        \n",
    "        \n",
    "\n",
    "        #####################     ALIGNMENT #################\n",
    "        print(f\"STARTING ALIGNMENT on {wav_file}\")\n",
    "        \n",
    "        # Load the alignment model with the specified device\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "        \n",
    "        # Perform alignment using the specified device\n",
    "        aligned_result = whisperx.align(transcribe_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        \n",
    "        # Save the Alignment result to a JSON file\n",
    "        alignment_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{alignment_filename}_aligned.json', 'w') as json_file:\n",
    "            json.dump(aligned_result, json_file, indent=4)\n",
    "        \n",
    "        # Append the aligned result to results_full\n",
    "        aligned_results_full.append(aligned_result)\n",
    "        \n",
    "        #####################     DIARIZE #################\n",
    "        print(f\"STARTING DIARIZE on {wav_file}\")\n",
    "\n",
    "        # Load the DIARIZE Model\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=TOKEN_ID, device=device)\n",
    "\n",
    "        # Load the audio data\n",
    "        audio_data = {\n",
    "            'waveform': torch.from_numpy(audio[None, :]),\n",
    "            'sample_rate': whisperx.audio.SAMPLE_RATE\n",
    "                    }\n",
    "        # Run the diarization model\n",
    "        diarize_segments = diarize_model(audio)\n",
    "\n",
    "        # add min/max number of speakers if known\n",
    "        diarize_model(audio, min_speakers=1, max_speakers=3)\n",
    "\n",
    "        # Assign speaker labels to words\n",
    "        diarize_result = whisperx.assign_word_speakers(diarize_segments, aligned_result)\n",
    "\n",
    "        ## SAVE the TRANSCRIPT\n",
    "        diarized_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{diarized_filename}_diarized.json', 'w') as json_file:\n",
    "            json.dump(diarize_result, json_file, indent=4)\n",
    "     \n",
    "       # Clean up memory after each file\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {wav_file}: {e}\")\n",
    "\n",
    "# Optionally, save the full results to a single JSON file\n",
    "with open('./outputs/Testing/full_alignment.json', 'w') as json_file:\n",
    "    json.dump(aligned_results_full, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - use the Terminal and CLI \n",
    "This seemed to work on a single file VERY fast! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from HF_token import TOKEN_ID\n",
    "# Set the path to your directory\n",
    "directory = \"./data/Testing/\"\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".wav\"):  # Check for .wav files\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Construct and run the whisperx command for each file\n",
    "        command = f\"whisperx {filepath} --model large-v2 --diarize --highlight_words True --hf_token {TOKEN_ID}\"\n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Approach 2B - add some subprocess to monitor progress\n",
    "\n",
    " This method is *best* as a) actually worked and b) provided insight into what is going on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from HF_token import TOKEN_ID\n",
    "\n",
    "# Set the path to your directory\n",
    "directory = \"data/\"\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".wav\"):  # Check for .wav files\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Print the filename to show progress\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        # Construct the whisperx command for each file\n",
    "        command = f\"whisperx {filepath} --model large-v2 --diarize --highlight_words True --hf_token {TOKEN_ID} --output_dir ./outputs\"\n",
    "        \n",
    "        # Run the command and capture real-time output\n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        \n",
    "        # Display real-time output from the command\n",
    "        for line in process.stdout:\n",
    "            print(line.decode().strip())\n",
    "        \n",
    "        process.wait()  # Wait for process to finish\n",
    "        \n",
    "        # Confirm completion for each file\n",
    "        print(f\"Completed file: {filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate the Diarized JSON files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_directory = 'outputs/'\n",
    "\n",
    "# Get a list of all JSON files in the directory\n",
    "json_files = glob.glob(os.path.join(json_directory, '*.json'))\n",
    "\n",
    "# Initialize a list to hold all DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through each JSON file and merge segments\n",
    "for json_file in json_files:\n",
    "\twith open(json_file, 'r') as file:\n",
    "\t\tdata = json.load(file)\n",
    "\t\t# Convert the \"segments\" part of the JSON data to a DataFrame\n",
    "\t\tdf = pd.DataFrame(data[\"segments\"])\n",
    "\t\tdf_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "diarized_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Export\n",
    "diarized_df.to_csv('./data/diarzed_output_no_names.csv')\n",
    "\n",
    "# Display the consolidated DataFrame\n",
    "diarized_df.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Speaker names to the labels\n",
    "\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_01', 'May')\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_02', 'Clarkson')\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_00', 'Hammond')\n",
    "\n",
    "diarized_df.head(100)\n",
    "\n",
    "# Export\n",
    "diarized_df.to_csv('./data/diarzed_output_named.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LDA (Latent Dirichlet Allocation) Preparation\n",
    "### Import the previously created json/csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import\n",
    "diarized_df = pd.read_csv('./data/diarzed_output_named.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.467</td>\n",
       "      <td>23.068</td>\n",
       "      <td>Hello, hello, and welcome.</td>\n",
       "      <td>[{'word': 'Hello,', 'start': 21.467, 'end': 21...</td>\n",
       "      <td>Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.108</td>\n",
       "      <td>24.029</td>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>[{'word': 'Thank', 'start': 23.108, 'end': 23....</td>\n",
       "      <td>Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.049</td>\n",
       "      <td>34.256</td>\n",
       "      <td>Now, as you know, the producers on this show l...</td>\n",
       "      <td>[{'word': 'Now,', 'start': 24.049, 'end': 24.1...</td>\n",
       "      <td>Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34.597</td>\n",
       "      <td>39.640</td>\n",
       "      <td>Then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[{'word': 'Then', 'start': 34.597, 'end': 34.7...</td>\n",
       "      <td>Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39.900</td>\n",
       "      <td>40.080</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>[{'word': 'Yeah.', 'start': 39.9, 'end': 40.08...</td>\n",
       "      <td>Hammond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>1042</td>\n",
       "      <td>269.865</td>\n",
       "      <td>271.447</td>\n",
       "      <td>We're through!</td>\n",
       "      <td>[{'word': \"We're\", 'start': 269.865, 'end': 27...</td>\n",
       "      <td>Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1043</td>\n",
       "      <td>273.209</td>\n",
       "      <td>277.514</td>\n",
       "      <td>Both our cars were flooded, but our guides wer...</td>\n",
       "      <td>[{'word': 'Both', 'start': 273.209, 'end': 273...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>1044</td>\n",
       "      <td>278.215</td>\n",
       "      <td>283.842</td>\n",
       "      <td>People of Surrey, if this happens to you, you ...</td>\n",
       "      <td>[{'word': 'People', 'start': 278.215, 'end': 2...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>1045</td>\n",
       "      <td>283.922</td>\n",
       "      <td>286.085</td>\n",
       "      <td>Well, the people of Botswana have a tip for you.</td>\n",
       "      <td>[{'word': 'Well,', 'start': 283.922, 'end': 28...</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>1046</td>\n",
       "      <td>299.427</td>\n",
       "      <td>299.951</td>\n",
       "      <td>Meanwhile...</td>\n",
       "      <td>[{'word': 'Meanwhile...', 'start': 299.427, 'e...</td>\n",
       "      <td>SPEAKER_04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    start      end  \\\n",
       "0              0   21.467   23.068   \n",
       "1              1   23.108   24.029   \n",
       "2              2   24.049   34.256   \n",
       "3              3   34.597   39.640   \n",
       "4              4   39.900   40.080   \n",
       "...          ...      ...      ...   \n",
       "1042        1042  269.865  271.447   \n",
       "1043        1043  273.209  277.514   \n",
       "1044        1044  278.215  283.842   \n",
       "1045        1045  283.922  286.085   \n",
       "1046        1046  299.427  299.951   \n",
       "\n",
       "                                                   text  \\\n",
       "0                            Hello, hello, and welcome.   \n",
       "1                                  Thank you very much.   \n",
       "2     Now, as you know, the producers on this show l...   \n",
       "3     Then they set unbelievably hard tasks to do to...   \n",
       "4                                                 Yeah.   \n",
       "...                                                 ...   \n",
       "1042                                     We're through!   \n",
       "1043  Both our cars were flooded, but our guides wer...   \n",
       "1044  People of Surrey, if this happens to you, you ...   \n",
       "1045   Well, the people of Botswana have a tip for you.   \n",
       "1046                                       Meanwhile...   \n",
       "\n",
       "                                                  words     speaker  \n",
       "0     [{'word': 'Hello,', 'start': 21.467, 'end': 21...    Clarkson  \n",
       "1     [{'word': 'Thank', 'start': 23.108, 'end': 23....    Clarkson  \n",
       "2     [{'word': 'Now,', 'start': 24.049, 'end': 24.1...    Clarkson  \n",
       "3     [{'word': 'Then', 'start': 34.597, 'end': 34.7...    Clarkson  \n",
       "4     [{'word': 'Yeah.', 'start': 39.9, 'end': 40.08...     Hammond  \n",
       "...                                                 ...         ...  \n",
       "1042  [{'word': \"We're\", 'start': 269.865, 'end': 27...    Clarkson  \n",
       "1043  [{'word': 'Both', 'start': 273.209, 'end': 273...  SPEAKER_03  \n",
       "1044  [{'word': 'People', 'start': 278.215, 'end': 2...  SPEAKER_03  \n",
       "1045  [{'word': 'Well,', 'start': 283.922, 'end': 28...  SPEAKER_03  \n",
       "1046  [{'word': 'Meanwhile...', 'start': 299.427, 'e...  SPEAKER_04  \n",
       "\n",
       "[1047 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = diarized_df.copy()\n",
    "# Preprocessing steps for LDA analysis\n",
    "\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_df.to_csv(\"preprocess_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "### Cant remember why needed this ... \n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>speaker</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.467</td>\n",
       "      <td>23.068</td>\n",
       "      <td>Hello, hello, and welcome.</td>\n",
       "      <td>[{'word': 'Hello,', 'start': 21.467, 'end': 21...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>hello hello and welcome</td>\n",
       "      <td>[hello, hello, welcome]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.108</td>\n",
       "      <td>24.029</td>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>[{'word': 'Thank', 'start': 23.108, 'end': 23....</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>thank you very much</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.049</td>\n",
       "      <td>34.256</td>\n",
       "      <td>Now, as you know, the producers on this show l...</td>\n",
       "      <td>[{'word': 'Now,', 'start': 24.049, 'end': 24.1...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>now as you know the producers on this show lik...</td>\n",
       "      <td>[know, producers, show, like, give, us, challe...</td>\n",
       "      <td>[know, producer, show, give, challenge, specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34.597</td>\n",
       "      <td>39.640</td>\n",
       "      <td>Then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[{'word': 'Then', 'start': 34.597, 'end': 34.7...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[set, unbelievably, hard, tasks, see, one, us,...</td>\n",
       "      <td>[set, unbelievably, hard, task, see, get, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39.900</td>\n",
       "      <td>40.080</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>[{'word': 'Yeah.', 'start': 39.9, 'end': 40.08...</td>\n",
       "      <td>Hammond</td>\n",
       "      <td>yeah</td>\n",
       "      <td>[yeah]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   start     end  \\\n",
       "0           0  21.467  23.068   \n",
       "1           1  23.108  24.029   \n",
       "2           2  24.049  34.256   \n",
       "3           3  34.597  39.640   \n",
       "4           4  39.900  40.080   \n",
       "\n",
       "                                                text  \\\n",
       "0                         Hello, hello, and welcome.   \n",
       "1                               Thank you very much.   \n",
       "2  Now, as you know, the producers on this show l...   \n",
       "3  Then they set unbelievably hard tasks to do to...   \n",
       "4                                              Yeah.   \n",
       "\n",
       "                                               words   speaker  \\\n",
       "0  [{'word': 'Hello,', 'start': 21.467, 'end': 21...  Clarkson   \n",
       "1  [{'word': 'Thank', 'start': 23.108, 'end': 23....  Clarkson   \n",
       "2  [{'word': 'Now,', 'start': 24.049, 'end': 24.1...  Clarkson   \n",
       "3  [{'word': 'Then', 'start': 34.597, 'end': 34.7...  Clarkson   \n",
       "4  [{'word': 'Yeah.', 'start': 39.9, 'end': 40.08...   Hammond   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0                            hello hello and welcome   \n",
       "1                                thank you very much   \n",
       "2  now as you know the producers on this show lik...   \n",
       "3  then they set unbelievably hard tasks to do to...   \n",
       "4                                               yeah   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                            [hello, hello, welcome]   \n",
       "1                                      [thank, much]   \n",
       "2  [know, producers, show, like, give, us, challe...   \n",
       "3  [set, unbelievably, hard, tasks, see, one, us,...   \n",
       "4                                             [yeah]   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0                                                 []  \n",
       "1                                      [thank, much]  \n",
       "2  [know, producer, show, give, challenge, specif...  \n",
       "3  [set, unbelievably, hard, task, see, get, good...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Below is a suggestion from gpt. \n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# 1. Remove emails, newline characters, and non-alphabetic characters\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['text'].str.replace(r'\\S+@\\S+', '', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'http\\S+|www\\S+', '', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'\\n', ' ', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# 2. Convert to lowercase\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.lower()\n",
    "\n",
    "# 3. Tokenize the text\n",
    "preprocessed_df['tokens'] = preprocessed_df['cleaned_text'].apply(lambda x: gensim.utils.simple_preprocess(x, deacc=True))\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "preprocessed_df['tokens'] = preprocessed_df['tokens'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "# 5. Lemmatize the tokens\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "preprocessed_df['lemmatized_tokens'] = preprocessed_df['tokens'].apply(lambda x: [token.lemma_ for token in nlp(\" \".join(x)) if token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]])\n",
    "\n",
    "# Display the preprocessed dataframe\n",
    "preprocessed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>speaker</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.467</td>\n",
       "      <td>23.068</td>\n",
       "      <td>Hello, hello, and welcome.</td>\n",
       "      <td>[{'word': 'Hello,', 'start': 21.467, 'end': 21...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>hello hello and welcome</td>\n",
       "      <td>[hello, hello, welcome]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.108</td>\n",
       "      <td>24.029</td>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>[{'word': 'Thank', 'start': 23.108, 'end': 23....</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>thank you very much</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.049</td>\n",
       "      <td>34.256</td>\n",
       "      <td>Now, as you know, the producers on this show l...</td>\n",
       "      <td>[{'word': 'Now,', 'start': 24.049, 'end': 24.1...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>now as you know the producers on this show lik...</td>\n",
       "      <td>[know, producers, show, like, give, us, challe...</td>\n",
       "      <td>[know, producer, show, give, challenge, specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34.597</td>\n",
       "      <td>39.640</td>\n",
       "      <td>Then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[{'word': 'Then', 'start': 34.597, 'end': 34.7...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[set, unbelievably, hard, tasks, see, one, us,...</td>\n",
       "      <td>[set, unbelievably, hard, task, see, get, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>100.860</td>\n",
       "      <td>102.060</td>\n",
       "      <td>It is a Lancia Beta.</td>\n",
       "      <td>[{'word': 'It', 'start': 100.86, 'end': 100.96...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>it is a lancia beta</td>\n",
       "      <td>[lancia, beta]</td>\n",
       "      <td>[beta]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    start      end  \\\n",
       "0            0   21.467   23.068   \n",
       "1            1   23.108   24.029   \n",
       "2            2   24.049   34.256   \n",
       "3            3   34.597   39.640   \n",
       "16          16  100.860  102.060   \n",
       "\n",
       "                                                 text  \\\n",
       "0                          Hello, hello, and welcome.   \n",
       "1                                Thank you very much.   \n",
       "2   Now, as you know, the producers on this show l...   \n",
       "3   Then they set unbelievably hard tasks to do to...   \n",
       "16                               It is a Lancia Beta.   \n",
       "\n",
       "                                                words   speaker  \\\n",
       "0   [{'word': 'Hello,', 'start': 21.467, 'end': 21...  Clarkson   \n",
       "1   [{'word': 'Thank', 'start': 23.108, 'end': 23....  Clarkson   \n",
       "2   [{'word': 'Now,', 'start': 24.049, 'end': 24.1...  Clarkson   \n",
       "3   [{'word': 'Then', 'start': 34.597, 'end': 34.7...  Clarkson   \n",
       "16  [{'word': 'It', 'start': 100.86, 'end': 100.96...  Clarkson   \n",
       "\n",
       "                                         cleaned_text  \\\n",
       "0                             hello hello and welcome   \n",
       "1                                 thank you very much   \n",
       "2   now as you know the producers on this show lik...   \n",
       "3   then they set unbelievably hard tasks to do to...   \n",
       "16                                it is a lancia beta   \n",
       "\n",
       "                                               tokens  \\\n",
       "0                             [hello, hello, welcome]   \n",
       "1                                       [thank, much]   \n",
       "2   [know, producers, show, like, give, us, challe...   \n",
       "3   [set, unbelievably, hard, tasks, see, one, us,...   \n",
       "16                                     [lancia, beta]   \n",
       "\n",
       "                                    lemmatized_tokens  \n",
       "0                                                  []  \n",
       "1                                       [thank, much]  \n",
       "2   [know, producer, show, give, challenge, specif...  \n",
       "3   [set, unbelievably, hard, task, see, get, good...  \n",
       "16                                             [beta]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a new DataFrame for each speaker\n",
    "May_df = preprocessed_df[preprocessed_df['speaker'] == 'May']\n",
    "Clarkson_df = preprocessed_df[preprocessed_df['speaker'] == 'Clarkson']\n",
    "Hammond_df = preprocessed_df[preprocessed_df['speaker'] == 'Hammond']\n",
    "\n",
    "# Display the first few rows of each DataFrame (optional)\n",
    "# May_df.head()\n",
    "\n",
    "Clarkson_df.head()\n",
    "\n",
    "# Hammond_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data frame into 3 (one per presenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>speaker</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.467</td>\n",
       "      <td>23.068</td>\n",
       "      <td>Hello, hello, and welcome.</td>\n",
       "      <td>[{'word': 'Hello,', 'start': 21.467, 'end': 21...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>hello hello and welcome</td>\n",
       "      <td>[hello, hello, welcome]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.108</td>\n",
       "      <td>24.029</td>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>[{'word': 'Thank', 'start': 23.108, 'end': 23....</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>thank you very much</td>\n",
       "      <td>[thank, much]</td>\n",
       "      <td>[thank, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.049</td>\n",
       "      <td>34.256</td>\n",
       "      <td>Now, as you know, the producers on this show l...</td>\n",
       "      <td>[{'word': 'Now,', 'start': 24.049, 'end': 24.1...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>now as you know the producers on this show lik...</td>\n",
       "      <td>[know, producers, show, like, give, us, challe...</td>\n",
       "      <td>[know, producer, show, give, challenge, specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34.597</td>\n",
       "      <td>39.640</td>\n",
       "      <td>Then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[{'word': 'Then', 'start': 34.597, 'end': 34.7...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>then they set unbelievably hard tasks to do to...</td>\n",
       "      <td>[set, unbelievably, hard, tasks, see, one, us,...</td>\n",
       "      <td>[set, unbelievably, hard, task, see, get, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>100.860</td>\n",
       "      <td>102.060</td>\n",
       "      <td>It is a Lancia Beta.</td>\n",
       "      <td>[{'word': 'It', 'start': 100.86, 'end': 100.96...</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>it is a lancia beta</td>\n",
       "      <td>[lancia, beta]</td>\n",
       "      <td>[beta]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    start      end  \\\n",
       "0            0   21.467   23.068   \n",
       "1            1   23.108   24.029   \n",
       "2            2   24.049   34.256   \n",
       "3            3   34.597   39.640   \n",
       "16          16  100.860  102.060   \n",
       "\n",
       "                                                 text  \\\n",
       "0                          Hello, hello, and welcome.   \n",
       "1                                Thank you very much.   \n",
       "2   Now, as you know, the producers on this show l...   \n",
       "3   Then they set unbelievably hard tasks to do to...   \n",
       "16                               It is a Lancia Beta.   \n",
       "\n",
       "                                                words   speaker  \\\n",
       "0   [{'word': 'Hello,', 'start': 21.467, 'end': 21...  Clarkson   \n",
       "1   [{'word': 'Thank', 'start': 23.108, 'end': 23....  Clarkson   \n",
       "2   [{'word': 'Now,', 'start': 24.049, 'end': 24.1...  Clarkson   \n",
       "3   [{'word': 'Then', 'start': 34.597, 'end': 34.7...  Clarkson   \n",
       "16  [{'word': 'It', 'start': 100.86, 'end': 100.96...  Clarkson   \n",
       "\n",
       "                                         cleaned_text  \\\n",
       "0                             hello hello and welcome   \n",
       "1                                 thank you very much   \n",
       "2   now as you know the producers on this show lik...   \n",
       "3   then they set unbelievably hard tasks to do to...   \n",
       "16                                it is a lancia beta   \n",
       "\n",
       "                                               tokens  \\\n",
       "0                             [hello, hello, welcome]   \n",
       "1                                       [thank, much]   \n",
       "2   [know, producers, show, like, give, us, challe...   \n",
       "3   [set, unbelievably, hard, tasks, see, one, us,...   \n",
       "16                                     [lancia, beta]   \n",
       "\n",
       "                                    lemmatized_tokens  \n",
       "0                                                  []  \n",
       "1                                       [thank, much]  \n",
       "2   [know, producer, show, give, challenge, specif...  \n",
       "3   [set, unbelievably, hard, task, see, get, good...  \n",
       "16                                             [beta]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a new DataFrame for each speaker\n",
    "May_df = diarized_df[diarized_df['speaker'] == 'May']\n",
    "Clarkson_df = diarized_df[diarized_df['speaker'] == 'Clarkson']\n",
    "Hammond_df = diarized_df[diarized_df['speaker'] == 'Hammond']\n",
    "\n",
    "# Display the first few rows of each DataFrame (optional)\n",
    "# May_df.head()\n",
    "\n",
    "Clarkson_df.head()\n",
    "\n",
    "# Hammond_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PREPROCESS\n",
    "Remove emails, newline char, stop words, and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello hello and welcome', 'thank you very much', 'now you know the producers this show like give challenges specifically where they give very small amount money and tell buy used car', 'then they set unbelievably hard tasks see which one got the best deal', 'lancia beta', '', 'the only lancia any sort the whole botswana', 'its done miles', 'one owner it', 'its the little old lady', 'and boy had she ragged it', 'no thats because the battery oh shorts the bonnet', 'shorts the bonnet bit', 'youve done well', 'what the hell that', 'dont know', 'could moskvitch', 'its opel', 'opel', 'and the front says cadet', 'yeah', 'thats the same age as thats the same age nick', 'love the speedo', 'like horizontal speedo', 'wheres the engine', 'with the cars the start line was time for our challenge', 'the people surrey think they need four wheel drive cars because they live lane which sometimes has leaves it', 'you will now attempt prove them wrong driving your two wheel drive cars from here botswanas eastern border with zimbabwe which there miles its western border with namibia', 'thats right across the spine africa', 'wasnt', 'hadnt even started and the lancia was playing up', 'james chose not wait', 'may mechanically confident but has just turned right which simply bomb wing which where should point out the bbc not allowed', 'sorry sorry', 'going now', 'the lancia wheezed into life and hammond and set off pursuit', 'what you have remember that three the most rugged and successful rally cars ever made were lancias', 'the stratos the and the delta integrale', 'lancia know how make rally car', 'unfortunately the day was made they obviously forgot everything', 'the gearbox broken the steerings broken the windows broken', 'fact all had problems', 'nevertheless because were tarmac roads decided give our cars shakedown', 'building speed now', 'when was made this car had horsepower', 'top speed miles hour', 'here go', 'going kilometres hour more', 'its now five past ten the morning and its starting get quite hot', 'ive got idea', 'oh yeah', 'thats better', 'why have you got pen', 'you tick them off when youve seen them', 'hornbill southern yellowbill', 'far the journey had been doddle', 'then the tarmac just sort stopped', 'shut up', 'hammond was tetchy because knew the price failure', 'anyone whose car broke down would have complete the journey beetle', 'collectively our least favourite car the world', 'oh yes', 'the punishment', 'come on oliver', 'what did you say', 'oh god', 'cars fire but very specific place', 'wow look that', 'its magnifying glass', 'you dont have piece cardboard', 'oh god', 'hes stalled', 'hes going down', 'cant open the door', 'oh god', 'please', 'come out', 'come on', 'float', 'float', 'float', 'oliver', 'this good time acknowledge that mine the only one that has worked consistently', 'no', 'this worrying', 'need home saturday', 'youve crossed botswana', 'now stuck second but second will do', 'obviously mine would keep going the other side the whole continent but you know come on ten miles please', 'that engine', 'thats car', 'mistakeable clatter air cooled engine', 'and was', 'sorry theres brakes', 'congratulations', 'thats astonishing', 'no well you dont buy second hand car surprised that still works', 'had rebuilt once day', 'yeah but youre right its rubbish', 'mercedes the best car', 'what', 'what have had do', 'ive had change take the entire body off', 'all right admit that its not entirely original but mechanical terms was perfect', 'you cannot break it', 'no no no no theyre fine', 'no really ive worked that out', 'this the old glass', 'very thin glass', 'have lost enough weight now', 'the next morning the edge the salt pans thought wed come under attack from bond villain but turned out the vice president botswana', 'that killer ride', 'its better than official rover and couple policemen motorbikes', 'was amazed hear what was being planned', 'ive just never known anybody across car', 'this should the first time think', 'oh really', 'oh really', 'that should interesting', 'you were smiling youve just stopped', 'buoyed the vice presidents optimism set off', 'ploughed on the little opel was going well', 'can feel the heat there', 'its nice', 'had close this one point just about that much and then pulls the air out and you just get fresh air through its nice', 'do actually', 'ive got say has been nice the piece', 'you know not having him around', 'back strength pressed on apart from jeremy being bill oddie occasionally', 'why havent you turned on', 'you turn off the batterys not going start again', 'turn off and start then', 'lets have some beautiful silence', 'yes', 'yes', 'trees', 'life', 'mate did you ever think that youd that', 'genuinely proud him', 'am', 'obviously they recognise truly classy car', 'hello', 'what stupid thing say', 'next was the panzer tank', 'most powerful longest best tyres longest', 'the hands african stig though was flying', 'that the latest long line pedigree lightened mercedes benz sports cars', 'stopwatch still running', 'seconds', 'maybe start the stopwatch', 'think was hurry decided not take your car', 'what', 'eventually found smoother track', 'whoa', 'and did detailed analysis the lancias conditioning', 'sit rep', 'which year was made', 'its just driven all the way from zimbabwe here without going road', 'like this', 'yeah', 'get out the way the pub', 'shall move for you', 'you ask politely will move it', 'what about the honey badger', 'the what', 'thats the least scary sounding animal the world', 'honey badger does not kill you eat you', 'tears off your testicles', 'why called honey badger', 'want that tarpaulin then', 'tarpaulin', 'would like tarpaulin roof for lancia beta coupe', 'because mercedes were very popular africa james soon found spare door and boot lid for his car', 'and while was looking for more bits hatched plan', 'look what ive got', 'now this will attract flies which will make his life unpleasant', 'yes', 'oh yeah thats whopper', 'thats whole lion there', 'oh the smell will really thatll beautiful', 'also attached cowbell the underside his car', 'but while were doing this was making merry with the paint', 'that afternoon left man and headed north the okavango delta', 'oh god', 'thanks', 'well help yourself brakes why not', 'ive lost skull', 'moon the left with animals with baby its back', 'what does hippo just before its about attack', 'oh look', 'oh look that', 'jeremy thats rubbish commentary', 'was looking through', 'thunderbird one the rescue', 'come on sinking the bows', 'coming', 'no', 'cant help you', 'come on', 'james low slow sports car', 'going down', 'thats good', 'yes', 'ive got water coming into car', 'ive got wet bottom', 'were through']\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(data):\n",
    "   # Remove emails\n",
    "    data = [re.sub(r'\\S+@\\S+', '', i) for i in data]\n",
    "\n",
    "    # Remove URLs\n",
    "    data = [re.sub(r'http\\S+|www\\S+|https\\S+', '', i, flags=re.MULTILINE) for i in data]\n",
    "\n",
    "    # Remove newline characters\n",
    "    data = [i.replace('\\n', ' ').replace('\\r', '').strip() for i in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [i.replace(\"'\", \"\") for i in data]\n",
    "\n",
    "        # Remove words less than 3 characters\n",
    "    data = [' '.join([word for word in i.split() if len(word) >= 3]) for i in data]\n",
    "\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    data = [' '.join([word.lower() for word in re.findall(r'\\b[a-zA-Z]+\\b', i)]) for i in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Convert 'text' column to list and then apply the preprocessing function to each dataframe\n",
    "\n",
    "May_data = preprocess_text(May_df['text'].values.tolist())\n",
    "Clarkson_data = preprocess_text(Clarkson_df['text'].values.tolist())\n",
    "Hammond_data = preprocess_text(Hammond_df['text'].values.tolist())\n",
    "\n",
    "# print(May_data)\n",
    "print(Clarkson_data)\n",
    "# print(Hammond_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yeah', 'there', 'were', 'just', 'two', 'conditions'], ['mustnt', 'four', 'wheel', 'drive', 'and', 'mustnt', 'built', 'any', 'way', 'off', 'road'], ['the', 'meeting', 'point', 'was', 'the', 'border', 'post', 'between', 'zimbabwe', 'and', 'botswana'], ['and', 'for', 'once', 'was', 'the', 'first', 'arrive'], ['now', 'youd', 'expect', 'ive', 'done', 'this', 'properly'], ['what', 'ive', 'got', 'mercedes', 'benz', 'car', 'that', 'africa', 'absolutely', 'adores', 'because', 'its', 'comfortable', 'its', 'rugged', 'its', 'dependable', 'and', 'frankly', 'the', 'other', 'two', 'have', 'brought', 'anything', 'other', 'than', 'one', 'these', 'along', 'theyre', 'idiots'], ['the', 'first', 'idiot', 'arrived'], ['can', 'you', 'open', 'the', 'door'], ['the', 'handles', 'broken'], ['yeah', 'thats', 'normal', 'isnt', 'it'], ['that', 'fizzing'], ['thats', 'yeah'], ['whats', 'the', 'piece', 'cardboard', 'for'], ['that', 'for', 'mopping', 'moisture'], ['so', 'now', 'what'], ['you', 'want', 'lift'], ['left', 'the', 'lancia', 'cool', 'down', 'because', 'hammond', 'was', 'arriving'], ['youve', 'both', 'been', 'idiots'], ['no'], ['brilliantly', 'interesting', 'brilliantly', 'stylish', 'but', 'stupid'], ['confident'], ['right', 'going', 'overtake', 'the', 'truck'], ['here', 'go'], ['hes', 'pulled', 'out', 'its', 'slipstream'], ['well', 'its', 'really', 'coming', 'up'], ['still', 'truck'], ['still', 'truck'], ['hammond', 'hows', 'going'], ['did', 'well', 'but', 'doesnt', 'matter', 'because', 'used', 'some', 'rifles'], ['shot', 'the', 'cars', 'and', 'the', 'waters', 'all', 'drained', 'out'], ['its', 'brilliant'], ['you', 'are', 'going', 'try', 'and', 'mend', 'this'], ['did'], ['oh', 'like', 'this', 'one'], ['its', 'sad', 'sad'], ['its', 'sad', 'sad', 'situation'], ['unfortunately', 'you', 'can', 'only', 'give', 'man', 'much', 'sympathy'], ['james', 'and', 'went', 'ahead', 'make', 'camp'], ['hammond', 'knew', 'that', 'the', 'morning', 'his', 'beloved', 'oliver', 'wasnt', 'fixed', 'hed', 'have', 'leave', 'him', 'behind'], ['got', 'our', 'bush', 'mechanic', 'bring', 'the', 'generator', 'down', 'and', 'worked', 'through', 'the', 'night'], ['morning', 'came', 'and', 'still', 'hammond'], ['what', 'happened', 'today'], ['mean', 'feeling', 'quite', 'like', 'explorer'], ['way'], ['that', 'technically', 'possible'], ['probably', 'not'], ['for', 'years', 'ive', 'never', 'been', 'speechless'], ['with', 'all', 'the', 'cars', 'defying', 'all', 'the', 'odds', 'began', 'our', 'final', 'push', 'the', 'border'], ['the', 'lancia', 'could', 'beat', 'that'], ['every', 'time', 'brake', 'spears', 'off', 'the', 'right', 'and', 'unable', 'steer', 'left', 'correct', 'that'], ['this', 'has', 'now', 'gone', 'from', 'being', 'nuisance', 'being', 'downright', 'dangerous'], ['had', 'that', 'feel', 'car', 'that', 'was', 'dying'], ['richard', 'actually', 'sympathised', 'because', 'hed', 'been', 'there'], ['for', 'james', 'vehicle'], ['not', 'going', 'give', 'in'], ['ill', 'push', 'it'], ['there', 'you', 'go'], ['this', 'object', 'lesson', 'for', 'the', 'owners', 'old', 'cars', 'everywhere'], ['you', 'can', 'drive', 'them', 'round', 'the', 'world'], ['come', 'on'], ['kilometres'], ['thats', 'near', 'damn', 'thousand', 'miles'], ['ive', 'still', 'got', 'half', 'car', 'left', 'and', 'very', 'bad', 'hair'], ['dont', 'believe', 'that'], ['oliver', 'just', 'skipping'], ['boom'], ['this', 'car', 'was', 'born', 'this'], ['meanwhile', 'unmodified', 'cadet', 'going', 'adjust', 'quarter', 'light', 'bit'], ['just', 'inch'], ['thats', 'better'], ['its', 'car', 'the', 'crab'], ['no', 'its', 'tracking', 'troentruve'], ['hes', 'worried', 'about', 'tracking', 'and', 'look', 'it'], ['knew', 'that', 'jeremy', 'would', 'eventually', 'catch', 'up', 'but', 'what', 'would', 'driving'], ['can', 'see', 'something', 'the', 'mirrors'], ['please', 'please', 'let', 'beetle'], ['with', 'their', 'convoy', 'back', 'the', 'road'], ['annoyingly', 'clarkson', 'got', 'the', 'lancet', 'going', 'again'], ['and', 'then', 'came', 'across', 'some', 'big', 'birds'], ['wow'], ['two', 'cows', 'three', 'cows'], ['personally', 'absolutely', 'delighted', 'because', 'think', 'the', 'macaddy', 'caddy', 'one', 'the', 'most', 'unpleasant', 'places', 'ive', 'ever', 'been'], ['its', 'just', 'big', 'bowl', 'dust'], ['hello', 'mate'], ['ive', 'got', 'goosebumps'], ['know', 'philip', 'larkin', 'poem', 'about', 'the', 'moon'], ['would', 'you', 'like', 'hear', 'it'], ['no'], ['hes', 'protecting', 'the', 'important', 'bits'], ['keen', 'get', 'going', 'the', 'three', 'fired', 'our', 'engines'], ['well', 'when', 'say', 'three'], ['annoyingly', 'couple', 'the', 'locals', 'did', 'know', 'what', 'do'], ['and', 'drove', 'african', 'stigs', 'rally', 'stage', 'which', 'was', 'dried', 'up', 'riverbed', 'few', 'miles', 'away'], ['with', 'packed', 'spectator', 'stands', 'oliver', 'went', 'first'], ['three', 'two'], ['three', 'miles', 'hour'], ['eminent'], ['come', 'on', 'joggle', 'it', 'stigs', 'cousin'], ['thats', 'not', 'dust', 'its', 'fire'], ['can', 'turn', 'off'], ['look', 'all'], ['once', 'again', 'the', 'broken', 'lance', 'here', 'was', 'fixed'], ['and', 'then', 'jeremy', 'arrived', 'with', 'some', 'woe', 'for', 'all', 'us'], ['what'], ['forget', 'some', 'it'], ['thats', 'it'], ['those', 'are', 'the', 'fuel', 'cans'], ['the', 'town', 'maun', 'was', 'about', 'miles', 'away', 'and', 'with', 'little', 'fuel', 'wed', 'have', 'there', 'the', 'crow', 'flies'], ['now', 'really', 'would', 'see', 'how', 'theyd', 'cope', 'off', 'road'], ['weve', 'got', 'try', 'and', 'keep', 'the', 'distance', 'down', 'save', 'what', 'fuel', 'weve', 'got'], ['once', 'again', 'the', 'year', 'old', 'opel', 'absolutely', 'shone'], ['with', 'our', 'precious', 'fuel', 'burning', 'away', 'carved', 'out', 'the', 'straightest', 'path', 'possible'], ['horn', 'doesnt', 'work'], ['clearly', 'have', 'get', 'myself', 'out'], ['going', 'make', 'rudimentary', 'temporary', 'road', 'for', 'back', 'wheel'], ['thank', 'god'], ['were', 'now', 'just', 'over', 'halfway', 'and', 'amazingly', 'our', 'cars', 'were', 'still', 'running'], ['all', 'them'], ['the', 'next', 'day', 'the', 'centre', 'maun', 'got', 'our', 'next', 'challenge'], ['the', 'glittering', 'golden', 'envelope'], ['you', 'will', 'drive', 'your', 'cars', 'namibia', 'through', 'the', 'okavango', 'delta'], ['thats', 'the', 'really', 'big', 'wildlife', 'place'], ['the', 'okavango', 'you', 'will', 'encounter', 'many', 'deadly', 'animals', 'including', 'lions', 'leopards', 'cheetahs', 'hyenas', 'wild', 'dogs', 'hippos', 'black', 'rhino', 'and', 'crocodile'], ['bird', 'snakes', 'shield', 'nosed', 'snakes', 'puff', 'adders', 'boomslang', 'cape', 'cobras', 'banded', 'cobras', 'black', 'mambas', 'black', 'widows', 'and', 'thick', 'tailed', 'scorpions'], ['order', 'protect', 'ourselves', 'from', 'the', 'lions', 'and', 'the', 'honey', 'badgers', 'jeremy', 'and', 'would', 'have', 'rebuild', 'our', 'cars'], ['but', 'because', 'wed', 'left', 'all', 'the', 'bits', 'the', 'other', 'side', 'the', 'salt', 'pans', 'had', 'use', 'whatever', 'could', 'find'], ['did', 'lion', 'eat', 'this'], ['who', 'see', 'about', 'the', 'corrugated', 'metal'], ['could', 'you', 'put', 'new', 'door', 'car'], ['come', 'and', 'have', 'look'], ['ill', 'show', 'you', 'what', 'mean'], ['its', 'not', 'sophisticated', 'metalwork', 'this', 'but'], ['can', 'anybody', 'else', 'smell', 'burning', 'car'], ['like', 'barbecue', 'smell'], ['neared', 'the', 'okavango', 'the', 'roads', 'became', 'rougher', 'and', 'rougher'], ['this', 'where', 'the', 'big', 'ben', 'comes', 'into', 'its', 'own', 'really', 'because', 'its', 'got', 'good', 'ground', 'clearance', 'nothing', 'bloody', 'hell'], ['so', 'just', 'summarise', 'viewers', 'went', 'through', 'the', 'gates', 'and', 'into', 'the', 'game', 'reserve', 'the', 'road', 'changed', 'again', 'for', 'the', 'worse'], ['very', 'soft', 'sand'], ['stop', 'our', 'cars', 'bogging', 'down', 'had', 'drive', 'fast', 'possible'], ['had', 'ourselves', 'another', 'rally', 'stage'], ['amazingly', 'even', 'jeremy', 'had', 'his', 'work', 'cut', 'out', 'keeping', 'with', 'the', 'opel'], ['monkeys'], ['giraffe', 'the', 'right'], ['there'], ['oh', 'is'], ['well', 'its', 'probably', 'whole', 'hippo'], ['its', 'just', 'the', 'rest', 'him', 'under', 'the', 'water'], ['james', 'took', 'over'], ['that', 'ones', 'lifting', 'its', 'paw', 'bit', 'like', 'dog', 'does'], ['paw'], ['hoof'], ['foot'], ['whatever', 'you', 'call', 'it'], ['hoof'], ['lets', 'see'], ['live', 'with', 'it'], ['wow', 'there', 'elephant', 'right', 'there'], ['soon', 'our', 'route', 'was', 'blocked', 'river', 'and', 'what', 'youre', 'supposed', 'wade', 'check', 'the', 'depth'], ['right'], ['jeremy', 'made', 'drive', 'until', 'got', 'bored'], ['here', 'now'], ['no'], ['agree', 'with', 'him', 'for', 'once', 'actually', 'because', 'that', 'reedy', 'stuff', 'must', 'mean', 'its', 'shallow'], ['ooh', 'hang', 'minute'], ['permission', 'say', 'cock'], ['its', 'coming', 'in']]\n",
      "[['hello', 'hello', 'and', 'welcome'], ['thank', 'you', 'very', 'much'], ['now', 'you', 'know', 'the', 'producers', 'this', 'show', 'like', 'give', 'challenges', 'specifically', 'where', 'they', 'give', 'very', 'small', 'amount', 'money', 'and', 'tell', 'buy', 'used', 'car'], ['then', 'they', 'set', 'unbelievably', 'hard', 'tasks', 'see', 'which', 'one', 'got', 'the', 'best', 'deal'], ['lancia', 'beta'], [], ['the', 'only', 'lancia', 'any', 'sort', 'the', 'whole', 'botswana'], ['its', 'done', 'miles'], ['one', 'owner', 'it'], ['its', 'the', 'little', 'old', 'lady'], ['and', 'boy', 'had', 'she', 'ragged', 'it'], ['no', 'thats', 'because', 'the', 'battery', 'oh', 'shorts', 'the', 'bonnet'], ['shorts', 'the', 'bonnet', 'bit'], ['youve', 'done', 'well'], ['what', 'the', 'hell', 'that'], ['dont', 'know'], ['could', 'moskvitch'], ['its', 'opel'], ['opel'], ['and', 'the', 'front', 'says', 'cadet'], ['yeah'], ['thats', 'the', 'same', 'age', 'as', 'thats', 'the', 'same', 'age', 'nick'], ['love', 'the', 'speedo'], ['like', 'horizontal', 'speedo'], ['wheres', 'the', 'engine'], ['with', 'the', 'cars', 'the', 'start', 'line', 'was', 'time', 'for', 'our', 'challenge'], ['the', 'people', 'surrey', 'think', 'they', 'need', 'four', 'wheel', 'drive', 'cars', 'because', 'they', 'live', 'lane', 'which', 'sometimes', 'has', 'leaves', 'it'], ['you', 'will', 'now', 'attempt', 'prove', 'them', 'wrong', 'driving', 'your', 'two', 'wheel', 'drive', 'cars', 'from', 'here', 'botswanas', 'eastern', 'border', 'with', 'zimbabwe', 'which', 'there', 'miles', 'its', 'western', 'border', 'with', 'namibia'], ['thats', 'right', 'across', 'the', 'spine', 'africa'], ['wasnt'], ['hadnt', 'even', 'started', 'and', 'the', 'lancia', 'was', 'playing', 'up'], ['james', 'chose', 'not', 'wait'], ['may', 'mechanically', 'confident', 'but', 'has', 'just', 'turned', 'right', 'which', 'simply', 'bomb', 'wing', 'which', 'where', 'should', 'point', 'out', 'the', 'bbc', 'not', 'allowed'], ['sorry', 'sorry'], ['going', 'now'], ['the', 'lancia', 'wheezed', 'into', 'life', 'and', 'hammond', 'and', 'set', 'off', 'pursuit'], ['what', 'you', 'have', 'remember', 'that', 'three', 'the', 'most', 'rugged', 'and', 'successful', 'rally', 'cars', 'ever', 'made', 'were', 'lancias'], ['the', 'stratos', 'the', 'and', 'the', 'delta', 'integrale'], ['lancia', 'know', 'how', 'make', 'rally', 'car'], ['unfortunately', 'the', 'day', 'was', 'made', 'they', 'obviously', 'forgot', 'everything'], ['the', 'gearbox', 'broken', 'the', 'steerings', 'broken', 'the', 'windows', 'broken'], ['fact', 'all', 'had', 'problems'], ['nevertheless', 'because', 'were', 'tarmac', 'roads', 'decided', 'give', 'our', 'cars', 'shakedown'], ['building', 'speed', 'now'], ['when', 'was', 'made', 'this', 'car', 'had', 'horsepower'], ['top', 'speed', 'miles', 'hour'], ['here', 'go'], ['going', 'kilometres', 'hour', 'more'], ['its', 'now', 'five', 'past', 'ten', 'the', 'morning', 'and', 'its', 'starting', 'get', 'quite', 'hot'], ['ive', 'got', 'idea'], ['oh', 'yeah'], ['thats', 'better'], ['why', 'have', 'you', 'got', 'pen'], ['you', 'tick', 'them', 'off', 'when', 'youve', 'seen', 'them'], ['hornbill', 'southern', 'yellowbill'], ['far', 'the', 'journey', 'had', 'been', 'doddle'], ['then', 'the', 'tarmac', 'just', 'sort', 'stopped'], ['shut', 'up'], ['hammond', 'was', 'tetchy', 'because', 'knew', 'the', 'price', 'failure'], ['anyone', 'whose', 'car', 'broke', 'down', 'would', 'have', 'complete', 'the', 'journey', 'beetle'], ['collectively', 'our', 'least', 'favourite', 'car', 'the', 'world'], ['oh', 'yes'], ['the', 'punishment'], ['come', 'on', 'oliver'], ['what', 'did', 'you', 'say'], ['oh', 'god'], ['cars', 'fire', 'but', 'very', 'specific', 'place'], ['wow', 'look', 'that'], ['its', 'magnifying', 'glass'], ['you', 'dont', 'have', 'piece', 'cardboard'], ['oh', 'god'], ['hes', 'stalled'], ['hes', 'going', 'down'], ['cant', 'open', 'the', 'door'], ['oh', 'god'], ['please'], ['come', 'out'], ['come', 'on'], ['float'], ['float'], ['float'], ['oliver'], ['this', 'good', 'time', 'acknowledge', 'that', 'mine', 'the', 'only', 'one', 'that', 'has', 'worked', 'consistently'], ['no'], ['this', 'worrying'], ['need', 'home', 'saturday'], ['youve', 'crossed', 'botswana'], ['now', 'stuck', 'second', 'but', 'second', 'will', 'do'], ['obviously', 'mine', 'would', 'keep', 'going', 'the', 'other', 'side', 'the', 'whole', 'continent', 'but', 'you', 'know', 'come', 'on', 'ten', 'miles', 'please'], ['that', 'engine'], ['thats', 'car'], ['mistakeable', 'clatter', 'air', 'cooled', 'engine'], ['and', 'was'], ['sorry', 'theres', 'brakes'], ['congratulations'], ['thats', 'astonishing'], ['no', 'well', 'you', 'dont', 'buy', 'second', 'hand', 'car', 'surprised', 'that', 'still', 'works'], ['had', 'rebuilt', 'once', 'day'], ['yeah', 'but', 'youre', 'right', 'its', 'rubbish'], ['mercedes', 'the', 'best', 'car'], ['what'], ['what', 'have', 'had', 'do'], ['ive', 'had', 'change', 'take', 'the', 'entire', 'body', 'off'], ['all', 'right', 'admit', 'that', 'its', 'not', 'entirely', 'original', 'but', 'mechanical', 'terms', 'was', 'perfect'], ['you', 'cannot', 'break', 'it'], ['no', 'no', 'no', 'no', 'theyre', 'fine'], ['no', 'really', 'ive', 'worked', 'that', 'out'], ['this', 'the', 'old', 'glass'], ['very', 'thin', 'glass'], ['have', 'lost', 'enough', 'weight', 'now'], ['the', 'next', 'morning', 'the', 'edge', 'the', 'salt', 'pans', 'thought', 'wed', 'come', 'under', 'attack', 'from', 'bond', 'villain', 'but', 'turned', 'out', 'the', 'vice', 'president', 'botswana'], ['that', 'killer', 'ride'], ['its', 'better', 'than', 'official', 'rover', 'and', 'couple', 'policemen', 'motorbikes'], ['was', 'amazed', 'hear', 'what', 'was', 'being', 'planned'], ['ive', 'just', 'never', 'known', 'anybody', 'across', 'car'], ['this', 'should', 'the', 'first', 'time', 'think'], ['oh', 'really'], ['oh', 'really'], ['that', 'should', 'interesting'], ['you', 'were', 'smiling', 'youve', 'just', 'stopped'], ['buoyed', 'the', 'vice', 'presidents', 'optimism', 'set', 'off'], ['ploughed', 'on', 'the', 'little', 'opel', 'was', 'going', 'well'], ['can', 'feel', 'the', 'heat', 'there'], ['its', 'nice'], ['had', 'close', 'this', 'one', 'point', 'just', 'about', 'that', 'much', 'and', 'then', 'pulls', 'the', 'air', 'out', 'and', 'you', 'just', 'get', 'fresh', 'air', 'through', 'its', 'nice'], ['do', 'actually'], ['ive', 'got', 'say', 'has', 'been', 'nice', 'the', 'piece'], ['you', 'know', 'not', 'having', 'him', 'around'], ['back', 'strength', 'pressed', 'on', 'apart', 'from', 'jeremy', 'being', 'bill', 'oddie', 'occasionally'], ['why', 'havent', 'you', 'turned', 'on'], ['you', 'turn', 'off', 'the', 'batterys', 'not', 'going', 'start', 'again'], ['turn', 'off', 'and', 'start', 'then'], ['lets', 'have', 'some', 'beautiful', 'silence'], ['yes'], ['yes'], ['trees'], ['life'], ['mate', 'did', 'you', 'ever', 'think', 'that', 'youd', 'that'], ['genuinely', 'proud', 'him'], ['am'], ['obviously', 'they', 'recognise', 'truly', 'classy', 'car'], ['hello'], ['what', 'stupid', 'thing', 'say'], ['next', 'was', 'the', 'panzer', 'tank'], ['most', 'powerful', 'longest', 'best', 'tyres', 'longest'], ['the', 'hands', 'african', 'stig', 'though', 'was', 'flying'], ['that', 'the', 'latest', 'long', 'line', 'pedigree', 'lightened', 'mercedes', 'benz', 'sports', 'cars'], ['stopwatch', 'still', 'running'], ['seconds'], ['maybe', 'start', 'the', 'stopwatch'], ['think', 'was', 'hurry', 'decided', 'not', 'take', 'your', 'car'], ['what'], ['eventually', 'found', 'smoother', 'track'], ['whoa'], ['and', 'did', 'detailed', 'analysis', 'the', 'lancias', 'conditioning'], ['sit', 'rep'], ['which', 'year', 'was', 'made'], ['its', 'just', 'driven', 'all', 'the', 'way', 'from', 'zimbabwe', 'here', 'without', 'going', 'road'], ['like', 'this'], ['yeah'], ['get', 'out', 'the', 'way', 'the', 'pub'], ['shall', 'move', 'for', 'you'], ['you', 'ask', 'politely', 'will', 'move', 'it'], ['what', 'about', 'the', 'honey', 'badger'], ['the', 'what'], ['thats', 'the', 'least', 'scary', 'sounding', 'animal', 'the', 'world'], ['honey', 'badger', 'does', 'not', 'kill', 'you', 'eat', 'you'], ['tears', 'off', 'your', 'testicles'], ['why', 'called', 'honey', 'badger'], ['want', 'that', 'tarpaulin', 'then'], ['tarpaulin'], ['would', 'like', 'tarpaulin', 'roof', 'for', 'lancia', 'beta', 'coupe'], ['because', 'mercedes', 'were', 'very', 'popular', 'africa', 'james', 'soon', 'found', 'spare', 'door', 'and', 'boot', 'lid', 'for', 'his', 'car'], ['and', 'while', 'was', 'looking', 'for', 'more', 'bits', 'hatched', 'plan'], ['look', 'what', 'ive', 'got'], ['now', 'this', 'will', 'attract', 'flies', 'which', 'will', 'make', 'his', 'life', 'unpleasant'], ['yes'], ['oh', 'yeah', 'thats', 'whopper'], ['thats', 'whole', 'lion', 'there'], ['oh', 'the', 'smell', 'will', 'really', 'thatll', 'beautiful'], ['also', 'attached', 'cowbell', 'the', 'underside', 'his', 'car'], ['but', 'while', 'were', 'doing', 'this', 'was', 'making', 'merry', 'with', 'the', 'paint'], ['that', 'afternoon', 'left', 'man', 'and', 'headed', 'north', 'the', 'okavango', 'delta'], ['oh', 'god'], ['thanks'], ['well', 'help', 'yourself', 'brakes', 'why', 'not'], ['ive', 'lost', 'skull'], ['moon', 'the', 'left', 'with', 'animals', 'with', 'baby', 'its', 'back'], ['what', 'does', 'hippo', 'just', 'before', 'its', 'about', 'attack'], ['oh', 'look'], ['oh', 'look', 'that'], ['jeremy', 'thats', 'rubbish', 'commentary'], ['was', 'looking', 'through'], ['thunderbird', 'one', 'the', 'rescue'], ['come', 'on', 'sinking', 'the', 'bows'], ['coming'], ['no'], ['cant', 'help', 'you'], ['come', 'on'], ['james', 'low', 'slow', 'sports', 'car'], ['going', 'down'], ['thats', 'good'], ['yes'], ['ive', 'got', 'water', 'coming', 'into', 'car'], ['ive', 'got', 'wet', 'bottom'], ['were', 'through']]\n",
      "[['yeah'], ['this', 'week', 'for', 'top', 'gear', 'special', 'they', 'came', 'with', 'real', 'humdinger'], ['they', 'gave', 'each', 'quid', 'and', 'told', 'africa', 'and', 'buy', 'car'], ['what', 'the', 'hell', 'have', 'you', 'done', 'man'], ['its', 'opel', 'cadet', 'from'], ['yes', 'but', 'its', 'much', 'better', 'nick', 'than', 'you'], ['was'], ['had', 'much', 'change', 'with', 'which', 'buy', 'many', 'beans'], ['how', 'much', 'more', 'simple', 'can', 'you', 'get'], ['its', 'got', 'two', 'moving', 'parts', 'and', 'its', 'been', 'here', 'for', 'years'], ['really', 'do'], ['its', 'there'], ['hang', 'on'], ['its', 'tiny'], ['you', 'want', 'know', 'about', 'the', 'power'], ['yes', 'do'], ['forty'], ['forty', 'horsepower'], ['well', 'they', 'did', 'sport', 'version', 'with', 'but', 'didnt', 'want', 'anything', 'too', 'lairy'], ['why', 'mine', 'stupid'], ['where', 'yours'], ['anansi', 'you', 'have', 'been', 'bit', 'thick'], ['you', 'think', 'the', 'cars', 'that', 'this', 'inspired', 'was', 'latin'], ['built', 'russia', 'where', 'became', 'the', 'moskvitch', 'which', 'was', 'rubbish'], ['and', 'course', 'indirectly', 'the', 'vauxhall', 'astra'], ['quite', 'lot', 'reasonably', 'average', 'cars', 'owe', 'themselves', 'this'], ['the', 'brakes', 'are', 'terrible', 'because', 'they', 'only', 'work', 'that', 'wheel'], ['they', 'work', 'very', 'well', 'that', 'wheel', 'but', 'only', 'that', 'wheel'], ['this', 'just', 'the', 'happiest', 'car', 'the', 'world'], ['call', 'it', 'oliver'], ['not', 'that', 'wed', 'ever', 'name', 'car', 'top', 'it'], ['wish', 'hadnt', 'said', 'that'], ['dont', 'know', 'what', 'that', 'is'], ['the', 'horn'], ['oliver', 'youve', 'got', 'cold'], ['listen'], ['were', 'having', 'fun', 'but', 'then', 'discovered', 'were', 'travelling', 'with', 'bill', 'oddie'], ['take', 'look', 'your', 'car'], ['its', 'massive'], ['dont', 'knock', 'oliver'], ['dont', 'knock', 'car'], ['thats', 'fine', 'hes', 'fine', 'what', 'did', 'you', 'call', 'it', 'then'], ['olivers', 'friend', 'mine', 'and', 'thought', 'you', 'were', 'talking', 'about', 'him'], ['hes', 'given', 'name'], ['hes', 'given', 'his', 'car', 'name'], ['come', 'on', 'oliver', 'please'], ['oh', 'god'], ['oh'], ['please'], ['oh', 'hang', 'on', 'hang', 'on'], ['said', 'love', 'you'], ['what', 'now'], ['sets', 'itself', 'fire'], ['starting', 'from', 'the', 'top', 'the', 'batterys', 'probably', 'pretty', 'knackered', 'and', 'then', 'the', 'carburettor', 'will', 'flooded', 'literal', 'sense'], ['and', 'then', 'working', 'down', 'the', 'distributor', 'cap', 'thatll', 'full', 'water'], ['the', 'engine', 'itself', 'mean', 'bit', 'water', 'went', 'into', 'the', 'cylinders', 'you', 'might', 'have', 'compressed', 'and', 'broken', 'something'], ['the', 'oil', 'will', 'ruined'], ['know', 'all', 'this'], ['oh', 'thats', 'harsh'], ['that', 'quite', 'harsh', 'but', 'on'], ['that', 'hammond'], ['beetle', 'more', 'the', 'point'], ['no', 'its', 'excellent'], ['car', 'working', 'perfectly', 'usual'], ['soon', 'reached', 'the', 'end', 'the', 'okavango', 'pulled', 'over', 'remove', 'the', 'animal', 'protection'], ['and', 'then', 'guess', 'what'], ['the', 'lancia', 'simply', 'would', 'not', 'get', 'going', 'again'], ['now', 'wont', 'start', 'because', 'the', 'starters', 'only', 'chosen', 'this', 'moment', 'pack', 'up', 'but', 'when', 'was', 'running', 'wouldnt', 'move', 'off', 'any', 'the', 'gears'], ['was', 'just', 'that', 'last'], ['tomorrow', 'night', 'its', 'more', 'from', 'the', 'top', 'gear', 'lads', 'with', 'bolivia', 'special', 'jeremy', 'james', 'and', 'richard', 'are', 'driving', 'all', 'the', 'way', 'the', 'coast', 'chile', 'from', 'the', 'heart', 'the', 'jungle'], ['lets', 'hope', 'they', 'bring', 'enough', 'glove', 'compartment', 'snacks'], ['and', 'next', 'its', 'sherlock', 'three'], ['drove', 'deeper', 'and', 'deeper', 'into', 'the', 'bush'], ['lancia'], ['yeah'], ['what', 'that'], ['looks', 'like', 'the', 'sea'], ['eventually', 'the', 'road', 'disappeared', 'altogether'], ['people', 'surrey', 'hope', 'youre', 'watching', 'this'], ['are'], ['driving', 'lancia', 'beta', 'coupe', 'well', 'just', 'the', 'middle', 'the', 'whatever', 'you', 'call', 'this'], ['the', 'good', 'news', 'was', 'wed', 'successfully', 'reached', 'our', 'campsite', 'for', 'the', 'night'], ['the', 'bad', 'news', 'came', 'the', 'shape', 'another', 'challenge'], ['stretching', 'before', 'you', 'the', 'mercati', 'caddy'], ['these', 'are', 'the', 'biggest', 'salt', 'flats', 'the', 'world'], ['theyre', 'almost', 'completely', 'lifeless', 'and', 'wide', 'portugal'], ['car', 'has', 'ever', 'driven', 'across', 'them'], ['you', 'run', 'out', 'water', 'you', 'will', 'die'], ['your', 'car', 'breaks', 'down', 'and', 'you', 'cant', 'rescued', 'you', 'will', 'die'], ['you', 'run', 'out', 'food', 'you', 'will', 'die'], ['its', 'like', 'driving', 'creme', 'brulee'], ['theres', 'primeval', 'ooze', 'covered', 'with', 'thin', 'layer', 'salty', 'crust'], ['you', 'have', 'thin', 'tyres', 'you', 'will', 'break', 'through', 'that', 'crust', 'get', 'stuck', 'and', 'you', 'will', 'die'], ['advises', 'fit', 'fat', 'tyres', 'and', 'remove', 'much', 'weight', 'possible', 'before', 'setting', 'off'], ['well', 'how', 'hard', 'can', 'be'], ['dont', 'say', 'that'], ['camp', 'the', 'weight', 'shedding', 'began'], ['ready'], ['that', 'doesnt', 'work'], ['thanks', 'awfully'], ['feel', 'how', 'much', 'this', 'seat', 'weighs'], ['right', 'you', 'work'], ['can', 'point', 'something', 'out'], ['what'], ['hammonds', 'walking', 'around', 'his', 'car', 'muttering', 'about', 'how', 'needs', 'all', 'it'], ['know', 'exactly', 'what', 'hes', 'doing'], ['hes', 'formed', 'emotional', 'attachment', 'hasnt', 'he'], ['would', 'like', 'saying', 'him', 'could', 'you', 'cut', 'bits', 'off', 'your', 'wife'], ['what', 'are', 'you', 'doing'], ['just', 'going', 'take', 'the', 'radiator', 'grill', 'off', 'the', 'basis', 'that', 'its', 'just', 'ornament'], ['can', 'help', 'you'], ['please', 'do'], ['you', 'want', 'your', 'windows', 'there'], ['glasses'], ['very', 'thin'], ['god', 'they', 'come', 'off', 'easily'], ['james'], ['yes'], ['using', 'nothing', 'but', 'hammer'], ['yes'], ['you', 'havent', 'lost', 'ounce'], ['ive', 'lost', 'the', 'spare', 'wheel', 'and', 'something', 'else'], ['now', 'look', 'mine'], ['going', 'need', 'some', 'guide', 'ropes', 'stop', 'floating', 'away', 'like', 'big', 'balloon'], ['tomorrow', 'die', 'then'], ['no', 'think', 'its', 'like', 'all', 'these', 'things', 'its', 'exaggerated'], ['well', 'fine', 'honestly'], ['were', 'going', 'fine'], ['weve', 'done', 'enough'], ['yes'], ['nothing', 'really', 'prepares', 'you', 'for', 'the', 'sheer', 'size', 'these'], ['think', 'way', 'that', 'more', 'frightening', 'than', 'the', 'pole'], ['its', 'absolutely', 'you', 'can', 'see', 'the', 'curvature', 'the', 'earth'], ['sadly', 'though', 'despite', 'the', 'weight', 'shedding', 'lancia', 'was', 'not', 'doing', 'well'], ['judging', 'the', 'way', 'the', 'tyres', 'are', 'kind', 'turning'], ['digging', 'driving', 'along'], ['think', 'little', 'bit', 'more', 'has', 'come', 'out'], ['still', 'you', 'can', 'see', 'from', 'the', 'tyres', 'grooves', 'was', 'doing', 'better', 'than', 'the', 'merc'], ['looking', 'jamess', 'rear', 'wheel', 'and', 'hes', 'digging', 'long', 'way'], ['tried', 'help', 'him', 'along'], ['its', 'really', 'helpful'], ['well', 'hes', 'always', 'the', 'beetle', 'james'], ['its', 'waiting', 'for', 'you'], ['what', 'are', 'you', 'going', 'do'], ['its', 'sinking'], ['honestly', 'weve', 'done', 'how', 'far', 'that', 'mile'], ['its', 'there'], ['lancia', 'waded', 'again'], ['just', 'nudge'], ['thats', 'crash'], ['theres', 'thing'], ['unfortunately', 'because', 'was', 'automatic', 'was', 'useless'], ['told', 'richard', 'try'], ['but', 'didnt', 'want', 'hurt', 'oliver'], ['this', 'was', 'hopeless', 'had', 'rope', 'the', 'camera', 'crew'], ['three', 'two', 'one', 'because', 'the', 'ooze', 'was', 'bad', 'had', 'get', 'even', 'more', 'drastic', 'with', 'the', 'weight', 'shedding'], ['toiled', 'away', 'for', 'hours'], ['well', 'two', 'did'], ['and', 'then', 'finally', 'were', 'ready'], ['now', 'this', 'light'], ['lancia', 'beta', 'coupe', 'superleggera'], ['not', 'modification'], ['this', 'excellent'], ['why', 'dont', 'all', 'cars', 'have', 'doors'], ['when', 'come', 'power', 'going', 'make', 'rule', 'because', 'this', 'just', 'better'], ['however', 'the', 'mercati', 'caddy', 'was', 'not', 'going', 'let', 'off', 'that', 'lightly', 'and', 'soon', 'even', 'our', 'super', 'lightweight', 'cars', 'started', 'struggle', 'again'], ['come', 'on', 'just', 'oh'], ['oh'], ['oh', 'yes'], ['no', 'this', 'isnt', 'good'], ['oh', 'no'], ['only', 'the', 'opel', 'remained', 'trouble', 'free', 'which', 'was', 'bad', 'news', 'for', 'me'], ['yes'], ['come', 'on', 'man'], ['ill', 'forward', 'sam', 'and', 'then', 'well', 'again'], ['hang', 'on'], ['how', 'far', 'it'], ['another', 'five', 'six', 'yards', 'and', 'might', 'bobbing'], ['oh', 'this', 'just', 'horrible'], ['keep', 'going', 'keep', 'going', 'keep', 'going'], ['keep', 'going'], ['yes'], ['no'], ['james', 'dont', 'there'], ['youll', 'get', 'stuck'], ['thats', 'close'], ['oh', 'for', 'goodness', 'sake'], ['have', 'everybody', 'well', 'push', 'off'], ['need', 'men', 'more'], ['this', 'hopeless'], ['people', 'surrey', 'you', 'need', 'four', 'wheel', 'drive', 'for', 'this', 'bit'], ['the', 'gunk', 'was', 'sticky', 'had', 'completely', 'jammed', 'the', 'lancias', 'rear', 'wheels'], ['cant', 'describe', 'it'], ['you', 'just', 'think', 'its', 'just', 'mud'], ['you', 'know', 'what', 'is'], ['fish'], ['its', 'just', 'rotted', 'prehistoric', 'fish'], ['one', 'two', 'three'], ['mercifully', 'the', 'ground', 'eventually', 'hardened', 'and', 'made', 'good', 'progress'], ['but', 'then', 'suddenly', 'the', 'horizon', 'was', 'longer', 'flat'], ['now', 'this', 'interesting', 'because', 'were', 'now', 'coming', 'between', 'what', 'look', 'like', 'islands'], ['and', 'suppose', 'they', 'are'], ['this', 'was', 'lake', 'they', 'would', 'have', 'been', 'islands'], ['amazing'], ['absolutely', 'amazing'], ['where', 'are', 'we'], ['its', 'called', 'kubu', 'island'], ['were', 'about'], ['third', 'the', 'way', 'across'], ['and', 'youve', 'been', 'stuck', 'about', 'thousand', 'times'], ['that', 'smug'], ['baobab', 'tree'], ['always', 'wanted', 'see', 'one', 'those'], ['hammond', 'look', 'this'], ['whoa'], ['just', 'about', 'the', 'most', 'astonishing', 'place', 'ive', 'ever', 'been'], ['kind', 'witty', 'there'], ['the', 'sun', 'set', 'headed', 'for', 'the', 'campsite'], ['crikey', 'cant', 'even', 'see', 'jezza', 'already'], ['the', 'kakais', 'come', 'off'], ['please', 'let', 'beetle'], ['hope', 'its', 'beetle'], ['please', 'let', 'beetle'], ['back'], ['oh', 'jeremy', 'well', 'done'], ['disappointed', 'sorry', 'delighted'], ['this', 'where', 'flamingos', 'breed', 'out', 'here'], ['thats', 'breeding', 'it'], ['its', 'fossil'], ['its', 'not', 'fossil'], ['only', 'died', 'about', 'two', 'years', 'ago'], ['was', 'talking', 'about', 'you'], ['its', 'ex', 'flamingo'], ['hey', 'no', 'look'], ['what', 'now'], ['know', 'what', 'created', 'this'], ['ostrich'], ['yes'], ['its', 'legs', 'are', 'miles', 'apart'], ['thats', 'exactly', 'how', 'was', 'walking'], ['you', 'know', 'david', 'attenboroughs', 'about', 'retire'], ['can', 'just', 'say', 'what'], ['you', 'look', 'like', 'gay', 'cowboy', 'and', 'you', 'look', 'like', 'gay', 'terrorist'], ['no', 'you', 'look', 'like', 'terrorist', 'with', 'broken', 'windscreen', 'wiper', 'and', 'your', 'face', 'ridiculous'], ['ern', 'hammond', 'started', 'pick', 'car'], ['its', 'gathering', 'electricity'], ['yeah', 'on'], ['turn', 'off', 'and', 'start', 'it'], ['are', 'you', 'ready'], ['are', 'you', 'ready'], ['behold', 'why', 'did', 'you', 'turn', 'off', 'you', 'idiot'], ['because', 'said', 'would', 'good', 'luck', 'mate'], ['somebody', 'will', 'give', 'you', 'dub', 'stop'], ['dont', 'away'], ['goodbye'], ['had', 'crossed', 'the', 'mercati', 'caddy'], ['really', 'startled', 'that', 'this', 'that', 'isnt', 'car', 'anymore', 'whatever', 'is', 'has', 'done', 'it'], ['with', 'its', 'low', 'profile', 'tyres', 'and', 'its', 'low', 'suspension', 'thats', 'broken'], ['widow', 'twankey', 'may', 'have', 'been', 'glad', 'see', 'the', 'back', 'the', 'salt', 'pans', 'but', 'despite', 'this', 'they', 'gave', 'startling', 'parting', 'gift'], ['wow', 'that', 'amazing'], ['thats', 'the', 'moon'], ['yeah', 'thats', 'the', 'moon'], ['because', 'the', 'dust', 'from', 'the', 'pans', 'you', 'get', 'moonrise', 'orange'], ['new', 'day', 'dawned', 'our', 'cars', 'looked', 'like', 'wrecks'], ['but', 'their', 'ordeal', 'was', 'far', 'from', 'over'], ['yes', 'had', 'crossed', 'the', 'salt', 'pans', 'but', 'were', 'still', 'only', 'third', 'the', 'way', 'across', 'botswana'], ['and', 'now', 'were', 'about', 'enter', 'the', 'kalahari'], ['kalahari'], ['everyone', 'who', 'comes', 'the', 'kalahari', 'takes', 'away', 'different', 'memory', 'it'], ['savagery', 'the', 'simplicity', 'the', 'vast', 'heat'], ['me', 'think', 'bumpiness'], ['was', 'rough', 'hell', 'were', 'glad', 'when', 'news', 'came', 'through', 'that', 'were', 'stop', 'the', 'next', 'village'], ['what', 'could', 'they', 'possibly', 'have', 'mind', 'for', 'here'], ['was', 'another', 'challenge'], ['here', 'is'], ['your', 'cars', 'have', 'travelled', 'far', 'and', 'suffered', 'much'], ['yes', 'they', 'have'], ['will', 'now', 'discover', 'how', 'much', 'performance', 'theyve', 'lost', 'competition', 'against', 'the', 'clock', 'rally', 'special', 'stage'], ['mine', 'cant', 'have', 'lost', 'any', 'performance'], ['never', 'added', 'any'], ['and', 'not', 'going', 'ruin', 'mercedes', 'just', 'for', 'few', 'points'], ['youre', 'right', 'youre', 'not', 'because', 'youre', 'not', 'driving', 'it'], ['well', 'who', 'is'], ['some', 'say', 'hes', 'seen', 'the', 'lion', 'king', 'times', 'and', 'that', 'his', 'second', 'best', 'friend', 'cape', 'buffalo'], ['all', 'know', 'hes', 'not', 'the', 'stig', 'but', 'the', 'stigs', 'african', 'cousin'], ['hes', 'enjoying', 'that'], ['and', 'one', 'minute', 'seconds'], ['think', 'thats', 'good', 'benchmark', 'for', 'you', 'try', 'and', 'beat'], ['hell', 'have', 'difficulty', 'though'], ['hes', 'off'], ['and', 'maybe', 'started'], ['hey', 'jeremy'], ['youre', 'right', 'though'], ['your', 'engine'], ['its', 'canted'], ['guys', 'you', 'know', 'what', 'were', 'driving', 'through'], ['plants'], ['no', 'this', 'weed'], ['grows', 'locally', 'and', 'its', 'hallucinogenic'], ['what', 'did', 'you', 'say'], ['any', 'car', 'was', 'going', 'get', 'stuck', 'would', 'have', 'put', 'money', 'the', 'lance', 'here', 'but', 'no'], ['all', 'right', 'mate'], ['sorry'], ['its', 'all', 'broken'], ['darkness', 'fell', 'found', 'the', 'road', 'mamm'], ['but', 'this', 'stage', 'even', 'oliver', 'was', 'suffering'], ['cant', 'use', 'lights'], ['alternators', 'packed', 'all', 'full', 'dirt', 'and', 'dust'], ['put', 'the', 'lights', 'on', 'can', 'have', 'the', 'lights', 'but', 'not', 'the', 'engine', 'because', 'all', 'dies'], ['ive', 'actually', 'got', 'jonathan', 'who', 'brings', 'all', 'the', 'cameras', 'and', 'does', 'all', 'that', 'for', 'us', 'here', 'with', 'me', 'lighting', 'when', 'talk', 'you', 'with', 'the', 'torch', 'which', 'then', 'uses', 'light', 'the', 'road', 'ahead', 'when', 'not', 'talking', 'you'], ['finally', 'rolled', 'into', 'man'], ['yeah', 'thats', 'his', 'line'], ['they', 'are', 'grown', 'ups', 'honestly'], ['they', 'this', 'lot'], ['does', 'not'], ['because', 'thats', 'why', 'its', 'made', 'angry'], ['why', 'isnt', 'called', 'the', 'badger', 'death'], ['suppose', 'ive', 'had', 'practice', 'least', 'with', 'lion', 'drill'], ['oh', 'no', 'theres', 'lion', 'coming'], ['ah', 'what', 'shall', 'do'], ['that'], ['oliver', 'will', 'protect', 'me'], ['because', 'had', 'nothing', 'do', 'decided', 'irritate', 'jeremy'], ['tarpaulin', 'will', 'defeat', 'oh', 'no', 'its', 'well', 'known', 'tarpaulin', 'for', 'its', 'almost', 'military', 'protective', 'capabilities'], ['thats', 'why', 'you', 'often', 'see', 'people', 'going', 'into', 'war', 'zones', 'draped', 'tarpaulin'], ['can'], ['can', 'make', 'anything', 'here'], ['you', 'have', 'any', 'lion', 'proof', 'tarpaulin'], ['its', 'got', 'be', 'like', 'this', 'thick'], ['yeah'], ['one', 'and', 'half', 'metres', 'by', 'say', 'one', 'and', 'half', 'metres'], ['you', 'think', 'lion', 'door', 'itd', 'about', 'that', 'big'], ['could', 'you', 'ignore', 'him', 'and', 'make', 'that'], ['thats', 'cows', 'head'], ['yeah'], ['will', 'also', 'attract', 'lions', 'tigers', 'effectively', 'becomes', 'burger', 'van', 'for', 'driving'], ['and', 'make', 'sure', 'the', 'lions', 'didnt', 'miss'], ['oh', 'yeah', 'under', 'the', 'seat'], ['right', 'car', 'now', 'has', 'been', 'readied'], ['you', 'can', 'see', 'have', 'wooden', 'door', 'here'], ['its', 'gull', 'wing', 'can', 'get', 'and', 'out', 'obviously'], ['god', 'this', 'like', 'being', 'allotment', 'shed', 'very', 'windy', 'day'], ['the', 'left', 'have', 'all', 'the', 'cans', 'that', 'were', 'the', 'car', 'for', 'the', 'last', 'few', 'days', 'have', 'been', 'arranged', 'and', 'fiesta', 'resistance'], ['badgers', 'away'], ['mad', 'jeremy'], ['police', 'car'], ['this', 'enough', 'shake', 'the', 'skulls', 'from', 'your', 'bonnet', 'and', 'nobody', 'the', 'whole', 'human', 'history', 'has', 'ever', 'said', 'that', 'before'], ['this', 'new'], ['hammonds', 'car', 'just', 'looks', 'composed'], ['developing', 'this', 'irrational', 'hatred', 'him', 'and', 'it'], ['this', 'such', 'good', 'game'], ['eventually', 'was', 'slowed', 'down', 'bridge', 'over', 'the', 'river', 'kwai'], ['the', 'rally', 'stage', 'had', 'taken', 'its', 'toll', 'one', 'the', 'cars'], ['take', 'guess', 'which', 'one'], ['car', 'which', 'even', 'probably', 'here', 'has', 'got', 'throttle', 'thats', 'jammed', 'wide', 'open', 'and', 'cant', 'hold', 'the', 'brakes'], ['hammond', 'move'], ['youre', 'going', 'have', 'faster', 'than', 'hit', 'you'], ['what'], ['sorry', 'cant', 'just', 'hit', 'the', 'throttle'], ['thats', 'tick', 'over'], ['look', 'legs', 'not', 'it'], ['having', 'blotched', 'throttle', 'headed', 'deeper', 'into', 'the', 'okavango'], ['front', 'oclock'], ['one', 'oclock'], ['this', 'where', 'wildlife', 'cameramen', 'come', 'make', 'name', 'for', 'themselves', 'with', 'david', 'attenborough'], ['but', 'unfortunately', 'our', 'film', 'crew', 'are', 'best', 'really', 'with', 'cars'], ['the', 'big', 'thing'], ['little', 'fluffy', 'clouds'], ['have', 'programme', 'called', 'the', 'back', 'end', 'animal'], ['meet', 'the', 'boys', 'hire'], ['this', 'week', 'too', 'late', 'look'], ['give', 'our', 'crew', 'chance', 'stopped', 'and', 'discovered', 'that', 'werent', 'much', 'good', 'animals', 'either'], ['wow', 'look'], ['look'], ['hippos', 'head'], ['opens', 'its', 'mouth'], ['opens', 'its', 'mouth'], ['can', 'open', 'its', 'jaw'], ['theres', 'thing', 'can', 'do'], ['they', 'stop', 'for', 'drink'], ['using', 'their', 'noses', 'shovel', 'water', 'into', 'their', 'mouths'], ['why', 'elephant'], ['its', 'amazing'], ['theres', 'man', 'over', 'there', 'with', 'the', 'best', 'comb', 'over', 'have', 'ever', 'seen', 'life'], ['that', 'is'], ['hes', 'got', 'four', 'partings', 'result', 'that'], ['why', 'dont', 'youre', 'bald'], ['oops', 'called', 'back'], ['jeremy'], ['jeremy'], ['what'], ['plugs'], ['that', 'sounded', 'very', 'close']]\n"
     ]
    }
   ],
   "source": [
    "#Define the function to tokenize:\n",
    "\n",
    "def send_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        # deacc=True remove punctuation\n",
    "\n",
    "# Define the datasets\n",
    "datasets_words = {\n",
    "    \"May_words\": May_data,\n",
    "    \"Clarkson_words\": Clarkson_data,\n",
    "    \"Hammond_words\": Hammond_data\n",
    "} \n",
    "\n",
    "# Apply the function to each dataset_words and store the results in new variables\n",
    "results_words = {}\n",
    "for name, data in datasets_words.items():\n",
    "    results_words[name] = list(send_to_words(data))\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_words = results_words[\"May_words\"]\n",
    "Clarkson_words = results_words[\"Clarkson_words\"]\n",
    "Hammond_words = results_words[\"Hammond_words\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(May_words)\n",
    "print(Clarkson_words)\n",
    "print(Hammond_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "*Function Definition:*\n",
    "\n",
    "The `send_to_words` function tokenizes sentences and removes punctuation.\n",
    "\n",
    "Ensure Data is Fully Loaded:\n",
    "The `ensure_iterable` function checks if the data is a LazyCorpusLoader and converts it to a list if necessary.\n",
    "Define Datasets:\n",
    "\n",
    "The `datasets_words` dictionary stores the original datasets with their corresponding names as keys.\n",
    "Apply Tokenization:\n",
    "\n",
    "A loop iterates through the datasets_words dictionary, applies the send_to_words function to each dataset, and converts the generator to a list before storing the results in a new dictionary named results_words.\n",
    "\n",
    "*Extract Results:*\n",
    "\n",
    "The results are extracted from the results_words dictionary into distinct variables for each dataset.\n",
    "Print Results:\n",
    "The results are printed to verify that the tokenization has been applied correctly.\n",
    "Check Iterability:\n",
    "\n",
    "The check_iterable function checks if the results are iterable and prints the result.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'May_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define the datasets\u001b[39;00m\n\u001b[0;32m     18\u001b[0m datasets_words \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMay_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mMay_data\u001b[49m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClarkson_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: Clarkson_data,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHammond_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: Hammond_data\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Apply the function to each dataset and store the results in new variables\u001b[39;00m\n\u001b[0;32m     25\u001b[0m results_words \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'May_data' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "# Define the function to tokenize\n",
    "def send_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        # deacc=True removes punctuation\n",
    "\n",
    "# Ensure the data is fully loaded before processing\n",
    "def ensure_iterable(data):\n",
    "    if isinstance(data, LazyCorpusLoader):\n",
    "        return list(data.words())  # Convert to list of words\n",
    "    return data\n",
    "\n",
    "# Define the datasets\n",
    "datasets_words = {\n",
    "    \"May_words\": May_data,\n",
    "    \"Clarkson_words\": Clarkson_data,\n",
    "    \"Hammond_words\": Hammond_data\n",
    "}\n",
    "\n",
    "# Apply the function to each dataset and store the results in new variables\n",
    "results_words = {}\n",
    "for name, data in datasets_words.items():\n",
    "    iterable_data = ensure_iterable(data)\n",
    "    results_words[name] = list(send_to_words(iterable_data))  # Convert generator to list\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_words = results_words[\"May_words\"]\n",
    "Clarkson_words = results_words[\"Clarkson_words\"]\n",
    "Hammond_words = results_words[\"Hammond_words\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(May_words)\n",
    "print(Clarkson_words)\n",
    "print(Hammond_words)\n",
    "\n",
    "# Check if the results are iterable\n",
    "def check_iterable(data):\n",
    "    try:\n",
    "        iter(data)\n",
    "        return True\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "print(f\"May_words is iterable: {check_iterable(May_words)}\")\n",
    "print(f\"Clarkson_words is iterable: {check_iterable(Clarkson_words)}\")\n",
    "print(f\"Hammond_words is iterable: {check_iterable(Hammond_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'May_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m gensim\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msimple_preprocess(\u001b[38;5;28mstr\u001b[39m(doc)) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define the datasets\u001b[39;00m\n\u001b[0;32m     13\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMay_data_words_nostops\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mMay_words\u001b[49m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClarkson_data_words_nostops\u001b[39m\u001b[38;5;124m\"\u001b[39m: Clarkson_words,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHammond_data_words_nostops\u001b[39m\u001b[38;5;124m\"\u001b[39m: Hammond_words\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Ensure the data is fully loaded before processing\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensure_iterable\u001b[39m(data):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'May_words' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# Define the stopwords list\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "# Define the function to remove stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "# Define the datasets\n",
    "datasets = {\n",
    "    \"May_data_words_nostops\": May_words,\n",
    "    \"Clarkson_data_words_nostops\": Clarkson_words,\n",
    "    \"Hammond_data_words_nostops\": Hammond_words\n",
    "}\n",
    "\n",
    "# Ensure the data is fully loaded before processing\n",
    "def ensure_iterable(data):\n",
    "    if isinstance(data, LazyCorpusLoader):\n",
    "        return list(data)\n",
    "    return data\n",
    "\n",
    "# Apply the function to each dataset and store the results in new variables\n",
    "results = {}\n",
    "for name, data in datasets.items():\n",
    "    print(f\"Processing {name}:\")\n",
    "    print(f\"Type: {type(data)}\")\n",
    "    print(f\"First 5 items: {data[:5] if isinstance(data, list) else 'Not a list'}\")\n",
    "    \n",
    "    iterable_data = ensure_iterable(data)\n",
    "    results[name] = remove_stopwords(iterable_data)\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_data_words_nostops = results[\"May_data_words_nostops\"]\n",
    "Clarkson_data_words_nostops = results[\"Clarkson_data_words_nostops\"]\n",
    "Hammond_data_words_nostops = results[\"Hammond_data_words_nostops\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(\"May_data_words_nostops:\", May_data_words_nostops[:5])\n",
    "print(\"Clarkson_data_words_nostops:\", Clarkson_data_words_nostops[:5])\n",
    "print(\"Hammond_data_words_nostops:\", Hammond_data_words_nostops[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Lemmatize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Basic syntax for a single list.\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'May_data_words_nostops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lemmatized_May \u001b[38;5;241m=\u001b[39m lemmatization(\u001b[43mMay_data_words_nostops\u001b[49m)\n\u001b[0;32m      2\u001b[0m lemmatized_Clarkson \u001b[38;5;241m=\u001b[39m lemmatization(Clarkson_data_words_nostops)\n\u001b[0;32m      3\u001b[0m lemmatized_Hammond \u001b[38;5;241m=\u001b[39m lemmatization(Hammond_data_words_nostops)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'May_data_words_nostops' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatized_May = lemmatization(May_data_words_nostops)\n",
    "lemmatized_Clarkson = lemmatization(Clarkson_data_words_nostops)\n",
    "lemmatized_Hammond = lemmatization(Hammond_data_words_nostops)\n",
    "\n",
    "# lemmatized_May\n",
    "lemmatized_Clarkson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_May' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_texts\u001b[39m(lemmatized_texts):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m lemmatized_texts \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m May_cleaned_texts \u001b[38;5;241m=\u001b[39m clean_texts(\u001b[43mlemmatized_May\u001b[49m)\n\u001b[0;32m      9\u001b[0m Clarkson_cleaned_texts \u001b[38;5;241m=\u001b[39m clean_texts(lemmatized_Clarkson)\n\u001b[0;32m     10\u001b[0m Hammond_cleaned_texts \u001b[38;5;241m=\u001b[39m clean_texts(lemmatized_Hammond)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_May' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "## Eliminate all the null strings in the lemmatized docs:\n",
    "\n",
    "def clean_texts(lemmatized_texts):\n",
    "\treturn [doc for doc in lemmatized_texts if len(doc) > 0]\n",
    "\n",
    "May_cleaned_texts = clean_texts(lemmatized_May)\n",
    "Clarkson_cleaned_texts = clean_texts(lemmatized_Clarkson)\n",
    "Hammond_cleaned_texts = clean_texts(lemmatized_Hammond)\n",
    "\n",
    "# May_cleaned_texts\n",
    "# print(Clarkson_cleaned_texts)\n",
    "# print(Hammond_cleaned_texts)\n",
    "\n",
    "Clarkson_cleaned_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path for the output text file\n",
    "output_file_path = './data/Clarkson_cleaned_texts.txt'\n",
    "\n",
    "# Write the cleaned texts to the text file\n",
    "with open(output_file_path, 'w') as file:\n",
    "\tfor item in Clarkson_cleaned_texts:\n",
    "\t\tfile.write(\"%s\\n\" % ' '.join(item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create the Corpora, Dictionary & LDA\n",
    "\n",
    "There are 3 data frames, so create ensure consistent and efficient treatment, create a function to pass each df through the same series of steps. These include: \n",
    "\n",
    "1. create the dictionary of terms\n",
    "2. create the corpus (count of each dictionary term)\n",
    "3. create the LDA Model\n",
    "4. run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpora_and_LDA(data_sets, names, num_topics = 5):\n",
    "    results_dict = {}\n",
    "    for data, name in zip(data_sets, names):\n",
    "        # STEP 1 - Create dictionary\n",
    "        id2word = corpora.Dictionary(data)\n",
    "        print(f\"Dictionary for {name}:\")\n",
    "        print(id2word)\n",
    "\n",
    "        # STEP 2 - Create Corpus\n",
    "        texts = data\n",
    "\n",
    "        # STEP 3 - Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        print(f\"Corpus for {name}:\")\n",
    "        print(corpus)\n",
    "\n",
    "        # STEP 4- Create LDA Model\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "        print(f\"LDA Model for {name}:\")\n",
    "        print(lda_model)\n",
    "\n",
    "        # STEP 5 - Store in dictionary\n",
    "        results_dict[name] = {\n",
    "            'dictionary': id2word,\n",
    "            'corpus': corpus,\n",
    "            'lda_model': lda_model\n",
    "        }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# RESULTS_DICTIONARY - Create a dictionary of the data sets, their respective names, and the info created from the above function. \n",
    "\n",
    "data_sets = [May_cleaned_texts, Clarkson_cleaned_texts, Hammond_cleaned_texts]\n",
    "names = ['May', 'Clarkson', 'Hammond']\n",
    "results_dict = create_corpora_and_LDA(data_sets, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the keyword in the 5 topics\n",
    "# \n",
    "import pprint\n",
    "\n",
    "for name, result in results_dict.items():\n",
    "    print(f\"Results for {name}:\")\n",
    "    pprint.pprint(result['lda_model'].print_topics())\n",
    "\n",
    "\n",
    "# print(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]\n",
    "\n",
    "#We created 5 topics. You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LDA Topic Models for each Presenter\n",
    "\n",
    "### Create a Visualize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def visualize_lda_model(results_dict, dataset_name):\n",
    "    # Access the dictionary, corpus, and LDA model for the specified dataset\n",
    "    dictionary = results_dict[dataset_name]['dictionary']\n",
    "    corpus = results_dict[dataset_name]['corpus']\n",
    "    lda_model = results_dict[dataset_name]['lda_model']\n",
    "\n",
    "    # Prepare the visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='mmds', R=30)\n",
    "\n",
    "    # Display the visualization\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the LDA Visualization for a given Presenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LDA model for 'X'\n",
    "vis = visualize_lda_model(results_dict, 'Clarkson')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Evaluation - Coherence \n",
    "\n",
    "Topic coherence measures the average similarity between top words having the highest weights in a topic i.e relative distance between the top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name in results_dict.keys():\n",
    "\tlda_model = results_dict[name]['lda_model']\n",
    "\ttexts = eval(f\"lemmatized_{name}\")\n",
    "\tdictionary = results_dict[name]['dictionary']\n",
    "\t\n",
    "\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "\tcoherence_lda = coherence_model_lda.get_coherence()\n",
    "\tprint(f'Coherence Score for {name}: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Model Improvement - How many topics? \n",
    "\n",
    "## Define function to Iterate Coherence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to iterate\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "\tcoherence_values = []\n",
    "\tmodel_list = []\n",
    "\t\n",
    "\tfor name in names:\n",
    "\t\tdictionary = results_dict[name]['dictionary']\n",
    "\t\tcorpus = results_dict[name]['corpus']\n",
    "\t\ttexts = eval(f\"lemmatized_{name}\")\n",
    "\t\t\n",
    "\t\tfor num_topics in range(start, limit, step):\n",
    "\t\t\tmodel = gensim.models.LdaModel(corpus=corpus, num_topics=num_topics, random_state=100, chunksize=200, passes=10, per_word_topics=True, id2word=dictionary)\n",
    "\t\t\tmodel_list.append(model)\n",
    "\t\t\tcoherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "\t\t\tcoherence_values.append(coherencemodel.get_coherence())\n",
    "\t\n",
    "\treturn model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the function and print the results for each name in results_dict\n",
    "limit = 10\n",
    "start = 2\n",
    "step = 1\n",
    "\n",
    "for name in names:\n",
    "\tprint(f\"Results for {name}:\")\n",
    "\tmodel_list, coherence_values = compute_coherence_values(results_dict[name]['dictionary'], results_dict[name]['corpus'], eval(f\"lemmatized_{name}\"), limit, start, step)\n",
    "\tfor m, cv in zip(range(start, limit, step), coherence_values):\n",
    "\t\tprint(f\"Num Topics = {m}, Coherence Value = {cv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
