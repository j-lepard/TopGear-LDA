{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 2 - ARCHIVED APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Automatic Speech Recognition, Diarize and Label\n",
    "\n",
    "Environment = \"whisperx\"\n",
    "\n",
    "* Performance Benchmarks on local\n",
    "* GPU Benchmark: 0.09961056709289551 seconds\n",
    "* Memory Bandwidth Benchmark: 0.2920224666595459 seconds\n",
    "* CPU Benchmark: 13.046526432037354 seconds\n",
    "* Disk Write Benchmark: 2.3364615440368652 seconds\n",
    "* Disk Read Benchmark: 0.05882525444030762 seconds \\n\n",
    "  \n",
    "** all benchmarks are >> faster than Collab with the exception of Disk write.\n",
    "\n",
    "## Setup ⚙️\n",
    "Tested for PyTorch 2.0, Python 3.10 (use other versions at your own risk!)\n",
    "GPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the CTranslate2 documentation.\n",
    "\n",
    "1.  Create Python3.10 environment\n",
    "\n",
    "`conda create --name whisperx python=3.10`\n",
    "\n",
    "`conda activate whisperx`\n",
    "\n",
    "2. Install PyTorch, e.g. for Linux and Windows CUDA11.8:\n",
    "   \n",
    "conda install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "See other methods here.\n",
    "\n",
    "1. Install this repo\n",
    "\n",
    "`pip install git+https://github.com/m-bain/whisperx.git`\n",
    "\n",
    "If already installed, update package to most recent commit\n",
    "\n",
    "`pip install git+https://github.com/m-bain/whisperx.git --upgrade`\n",
    "\n",
    "## Post Setup - REQUIRED for DIARIZATION **Actually dont do this!!\n",
    "https://github.com/m-bain/whisperX/issues/499\n",
    "\n",
    "`pip install pyannote.audio==3.0.1`\n",
    "\n",
    "`pip uninstall onnxruntime`\n",
    "\n",
    "`pip install --force-reinstall onnxruntime-gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess initial audio file\n",
    "convert to Wav using ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "## 1 - Convert Mp3 to WAV.\n",
    "\n",
    "def convert_m4a_to_mp3(input_file, output_file):\n",
    "    try:\n",
    "        ffmpeg.input(input_file).output(output_file).run(overwrite_output=True)\n",
    "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Input/ output files and usage\n",
    "input_mp3 = './audio/Botswana_2007_Audio.mp3'  # Change this to your mp3 file path\n",
    "output_wav = './data/Botswana_2007_Audio.wav'  # Change this to your desired output wav file path\n",
    "\n",
    "convert_m4a_to_mp3(input_mp3, output_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "## Full file should be the input (2007 or 2024 file..)\n",
    "audio_file = \"./data/Botswana_2007_Audio.wav\"\n",
    "\n",
    "## DEBUGGING, use a small file\n",
    "# audio_file = \"./data/Intro.wav\"\n",
    "\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "without_timestamps= 'True'\n",
    "\n",
    "## Some error handling to ensure that successfully loaded the mp3 file!\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"The file '{audio_file}' does not exist.\")\n",
    "    # Optionally, you can add more checks (like file format) here\n",
    "\n",
    "    print(f\"Successfully accessed the audio file: {audio_file}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "## Full file should be the input (2007 or 2024 file..)\n",
    "audio_file = \"./data/Botswana_2007_Audio.wav\"\n",
    "\n",
    "## DEBUGGING, use a small file\n",
    "# audio_file = \"./data/Intro.wav\"\n",
    "\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "without_timestamps= 'True'\n",
    "\n",
    "## Some error handling to ensure that successfully loaded the mp3 file!\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"The file '{audio_file}' does not exist.\")\n",
    "    # Optionally, you can add more checks (like file format) here\n",
    "\n",
    "    print(f\"Successfully accessed the audio file: {audio_file}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batches\n",
    "\n",
    "### Split the Audio file into smaller pieces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 - Split up large files in <10min\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import math\n",
    "\n",
    "\n",
    "# TODO: change max duration to 300 seconds\n",
    "# TODO: update the target database folder\n",
    "# TODO: check input filenames\n",
    "# TODO: Update the output folder\n",
    "\n",
    "# Function to split audio and save to database\n",
    "def split_audio(audio_file, max_duration=300):  # 60second (1min) for testing; 300sec for production\n",
    "    conn = sqlite3.connect('./data/Audio_clips.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS clips\n",
    "                     (id INTEGER PRIMARY KEY AUTOINCREMENT, start_time REAL, end_time REAL, filename TEXT)''')\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file: {e}\")\n",
    "        return []\n",
    "\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    num_splits = math.ceil(total_duration / max_duration)\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        start_time = i * max_duration\n",
    "        end_time = min((i + 1) * max_duration, total_duration)\n",
    "\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "\n",
    "        clip = y[start_sample:end_sample]\n",
    "        filename = f\"./data/Botswana2007_clip_{i}.wav\"\n",
    "\n",
    "        try:\n",
    "            sf.write(filename, clip, sr)\n",
    "            cursor.execute(\"INSERT INTO clips (start_time, end_time, filename) VALUES (?, ?, ?)\",\n",
    "                           (start_time, end_time, filename))\n",
    "            conn.commit()\n",
    "            results.append({\"start_time\": start_time, \"end_time\": end_time, \"filename\": filename})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing clip {i}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "    return results\n",
    "# results is a DIctionary\n",
    "results = split_audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH PROCESS: Transcript - Align - Diarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - use python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "from HF_token import TOKEN_ID\n",
    "\n",
    "# Directory containing .wav files\n",
    "wav_directory = './data/Testing'\n",
    "\n",
    "# Get a list of all .wav files in the directory\n",
    "wav_files = glob.glob(os.path.join(wav_directory, '*.wav'))\n",
    "\n",
    "# Initialize results_full list\n",
    "aligned_results_full = []\n",
    "\n",
    "# Set device and compute type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"float32\"\n",
    "\n",
    "print(f\"TRANSCRIBING & ALIGNING using device: {device}\")\n",
    "print(f\"Compute type is {compute_type}\")\n",
    "\n",
    "# Ensure the model directory exists\n",
    "model_dir = \"./model/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16  # Adjust as needed\n",
    "\n",
    "# # Load the model and save it to the local path\n",
    "# try:\n",
    "#     model = whisperx.load_model(\"large-v2\", device=device, compute_type=compute_type, download_root=model_dir)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     raise\n",
    "\n",
    "# Iterate through each .wav file and process it\n",
    "for wav_file in wav_files:\n",
    "    print(f\"Processing file: {wav_file}\")\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio = whisperx.load_audio(wav_file)\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        output_dir = \"./outputs/Testing\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        #####################     TRANSCRIPTION  #################\n",
    "        # Load the model and save it to the local path\n",
    "        try:\n",
    "            model = whisperx.load_model(\"large-v2\", device=device, compute_type=compute_type, download_root=model_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "        print(f\"STARTING Transcription on {wav_file}\")\n",
    "\n",
    "        # Transcribe the audio file\n",
    "        transcribe_result = model.transcribe(audio, batch_size=batch_size)\n",
    "        print(transcribe_result[\"segments\"])  # before alignment\n",
    "        \n",
    "        # Save the transcription result to a JSON file\n",
    "        transcript_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{transcript_filename}_transcript.json', 'w') as json_file:\n",
    "            json.dump(transcribe_result, json_file, indent=4)\n",
    "        \n",
    "        \n",
    "\n",
    "        #####################     ALIGNMENT #################\n",
    "        print(f\"STARTING ALIGNMENT on {wav_file}\")\n",
    "        \n",
    "        # Load the alignment model with the specified device\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "        \n",
    "        # Perform alignment using the specified device\n",
    "        aligned_result = whisperx.align(transcribe_result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        \n",
    "        # Save the Alignment result to a JSON file\n",
    "        alignment_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{alignment_filename}_aligned.json', 'w') as json_file:\n",
    "            json.dump(aligned_result, json_file, indent=4)\n",
    "        \n",
    "        # Append the aligned result to results_full\n",
    "        aligned_results_full.append(aligned_result)\n",
    "        \n",
    "        #####################     DIARIZE #################\n",
    "        print(f\"STARTING DIARIZE on {wav_file}\")\n",
    "\n",
    "        # Load the DIARIZE Model\n",
    "        diarize_model = whisperx.DiarizationPipeline(use_auth_token=TOKEN_ID, device=device)\n",
    "\n",
    "        # Load the audio data\n",
    "        audio_data = {\n",
    "            'waveform': torch.from_numpy(audio[None, :]),\n",
    "            'sample_rate': whisperx.audio.SAMPLE_RATE\n",
    "                    }\n",
    "        # Run the diarization model\n",
    "        diarize_segments = diarize_model(audio)\n",
    "\n",
    "        # add min/max number of speakers if known\n",
    "        diarize_model(audio, min_speakers=1, max_speakers=3)\n",
    "\n",
    "        # Assign speaker labels to words\n",
    "        diarize_result = whisperx.assign_word_speakers(diarize_segments, aligned_result)\n",
    "\n",
    "        ## SAVE the TRANSCRIPT\n",
    "        diarized_filename = os.path.basename(wav_file).replace('.wav', '')\n",
    "        with open(f'./outputs/Testing/{diarized_filename}_diarized.json', 'w') as json_file:\n",
    "            json.dump(diarize_result, json_file, indent=4)\n",
    "     \n",
    "       # Clean up memory after each file\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {wav_file}: {e}\")\n",
    "\n",
    "# Optionally, save the full results to a single JSON file\n",
    "with open('./outputs/Testing/full_alignment.json', 'w') as json_file:\n",
    "    json.dump(aligned_results_full, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - use the Terminal and CLI \n",
    "This seemed to work on a single file VERY fast! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from HF_token import TOKEN_ID\n",
    "# Set the path to your directory\n",
    "directory = \"./data/Testing/\"\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".wav\"):  # Check for .wav files\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Construct and run the whisperx command for each file\n",
    "        command = f\"whisperx {filepath} --model large-v2 --diarize --highlight_words True --hf_token {TOKEN_ID}\"\n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Approach 2B - add some subprocess to monitor progress\n",
    "\n",
    " This method is *best* as a) actually worked and b) provided insight into what is going on! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from HF_token import TOKEN_ID\n",
    "\n",
    "# Set the path to your directory\n",
    "directory = \"data/\"\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".wav\"):  # Check for .wav files\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Print the filename to show progress\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        \n",
    "        # Construct the whisperx command for each file\n",
    "        command = f\"whisperx {filepath} --model large-v2 --diarize --highlight_words True --hf_token {TOKEN_ID} --output_dir ./outputs\"\n",
    "        \n",
    "        # Run the command and capture real-time output\n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        \n",
    "        # Display real-time output from the command\n",
    "        for line in process.stdout:\n",
    "            print(line.decode().strip())\n",
    "        \n",
    "        process.wait()  # Wait for process to finish\n",
    "        \n",
    "        # Confirm completion for each file\n",
    "        print(f\"Completed file: {filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate the Diarized JSON files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_directory = 'outputs/'\n",
    "\n",
    "# Get a list of all JSON files in the directory\n",
    "json_files = glob.glob(os.path.join(json_directory, '*.json'))\n",
    "\n",
    "# Initialize a list to hold all DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through each JSON file and merge segments\n",
    "for json_file in json_files:\n",
    "\twith open(json_file, 'r') as file:\n",
    "\t\tdata = json.load(file)\n",
    "\t\t# Convert the \"segments\" part of the JSON data to a DataFrame\n",
    "\t\tdf = pd.DataFrame(data[\"segments\"])\n",
    "\t\tdf_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "diarized_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Export\n",
    "diarized_df.to_csv('./data/diarzed_output_no_names.csv')\n",
    "\n",
    "# Display the consolidated DataFrame\n",
    "diarized_df.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Speaker names to the labels\n",
    "\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_01', 'May')\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_02', 'Clarkson')\n",
    "diarized_df['speaker'] = diarized_df['speaker'].replace('SPEAKER_00', 'Hammond')\n",
    "\n",
    "diarized_df.head(100)\n",
    "\n",
    "# Export\n",
    "diarized_df.to_csv('./data/diarzed_output_named.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LDA (Latent Dirichlet Allocation) Preparation\n",
    "### Import the previously created json/csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import\n",
    "diarized_df = pd.read_csv('./data/diarzed_output_named.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = diarized_df.copy()\n",
    "# Preprocessing steps for LDA analysis\n",
    "\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_df.to_csv(\"preprocess_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "### Cant remember why needed this ... \n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Below is a suggestion from gpt. \n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# 1. Remove emails, newline characters, and non-alphabetic characters\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['text'].str.replace(r'\\S+@\\S+', '', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'http\\S+|www\\S+', '', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'\\n', ' ', regex=True)\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "\n",
    "# 2. Convert to lowercase\n",
    "preprocessed_df['cleaned_text'] = preprocessed_df['cleaned_text'].str.lower()\n",
    "\n",
    "# 3. Tokenize the text\n",
    "preprocessed_df['tokens'] = preprocessed_df['cleaned_text'].apply(lambda x: gensim.utils.simple_preprocess(x, deacc=True))\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "preprocessed_df['tokens'] = preprocessed_df['tokens'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "# 5. Lemmatize the tokens\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "preprocessed_df['lemmatized_tokens'] = preprocessed_df['tokens'].apply(lambda x: [token.lemma_ for token in nlp(\" \".join(x)) if token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]])\n",
    "\n",
    "# Display the preprocessed dataframe\n",
    "preprocessed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataFrame for each speaker\n",
    "May_df = preprocessed_df[preprocessed_df['speaker'] == 'May']\n",
    "Clarkson_df = preprocessed_df[preprocessed_df['speaker'] == 'Clarkson']\n",
    "Hammond_df = preprocessed_df[preprocessed_df['speaker'] == 'Hammond']\n",
    "\n",
    "# Display the first few rows of each DataFrame (optional)\n",
    "# May_df.head()\n",
    "\n",
    "Clarkson_df.head()\n",
    "\n",
    "# Hammond_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data frame into 3 (one per presenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataFrame for each speaker\n",
    "May_df = diarized_df[diarized_df['speaker'] == 'May']\n",
    "Clarkson_df = diarized_df[diarized_df['speaker'] == 'Clarkson']\n",
    "Hammond_df = diarized_df[diarized_df['speaker'] == 'Hammond']\n",
    "\n",
    "# Display the first few rows of each DataFrame (optional)\n",
    "# May_df.head()\n",
    "\n",
    "Clarkson_df.head()\n",
    "\n",
    "# Hammond_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PREPROCESS\n",
    "Remove emails, newline char, stop words, and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(data):\n",
    "   # Remove emails\n",
    "    data = [re.sub(r'\\S+@\\S+', '', i) for i in data]\n",
    "\n",
    "    # Remove URLs\n",
    "    data = [re.sub(r'http\\S+|www\\S+|https\\S+', '', i, flags=re.MULTILINE) for i in data]\n",
    "\n",
    "    # Remove newline characters\n",
    "    data = [i.replace('\\n', ' ').replace('\\r', '').strip() for i in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [i.replace(\"'\", \"\") for i in data]\n",
    "\n",
    "        # Remove words less than 3 characters\n",
    "    data = [' '.join([word for word in i.split() if len(word) >= 3]) for i in data]\n",
    "\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    data = [' '.join([word.lower() for word in re.findall(r'\\b[a-zA-Z]+\\b', i)]) for i in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Convert 'text' column to list and then apply the preprocessing function to each dataframe\n",
    "\n",
    "May_data = preprocess_text(May_df['text'].values.tolist())\n",
    "Clarkson_data = preprocess_text(Clarkson_df['text'].values.tolist())\n",
    "Hammond_data = preprocess_text(Hammond_df['text'].values.tolist())\n",
    "\n",
    "# print(May_data)\n",
    "print(Clarkson_data)\n",
    "# print(Hammond_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to tokenize:\n",
    "\n",
    "def send_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        # deacc=True remove punctuation\n",
    "\n",
    "# Define the datasets\n",
    "datasets_words = {\n",
    "    \"May_words\": May_data,\n",
    "    \"Clarkson_words\": Clarkson_data,\n",
    "    \"Hammond_words\": Hammond_data\n",
    "} \n",
    "\n",
    "# Apply the function to each dataset_words and store the results in new variables\n",
    "results_words = {}\n",
    "for name, data in datasets_words.items():\n",
    "    results_words[name] = list(send_to_words(data))\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_words = results_words[\"May_words\"]\n",
    "Clarkson_words = results_words[\"Clarkson_words\"]\n",
    "Hammond_words = results_words[\"Hammond_words\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(May_words)\n",
    "print(Clarkson_words)\n",
    "print(Hammond_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "*Function Definition:*\n",
    "\n",
    "The `send_to_words` function tokenizes sentences and removes punctuation.\n",
    "\n",
    "Ensure Data is Fully Loaded:\n",
    "The `ensure_iterable` function checks if the data is a LazyCorpusLoader and converts it to a list if necessary.\n",
    "Define Datasets:\n",
    "\n",
    "The `datasets_words` dictionary stores the original datasets with their corresponding names as keys.\n",
    "Apply Tokenization:\n",
    "\n",
    "A loop iterates through the datasets_words dictionary, applies the send_to_words function to each dataset, and converts the generator to a list before storing the results in a new dictionary named results_words.\n",
    "\n",
    "*Extract Results:*\n",
    "\n",
    "The results are extracted from the results_words dictionary into distinct variables for each dataset.\n",
    "Print Results:\n",
    "The results are printed to verify that the tokenization has been applied correctly.\n",
    "Check Iterability:\n",
    "\n",
    "The check_iterable function checks if the results are iterable and prints the result.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "# Define the function to tokenize\n",
    "def send_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        # deacc=True removes punctuation\n",
    "\n",
    "# Ensure the data is fully loaded before processing\n",
    "def ensure_iterable(data):\n",
    "    if isinstance(data, LazyCorpusLoader):\n",
    "        return list(data.words())  # Convert to list of words\n",
    "    return data\n",
    "\n",
    "# Define the datasets\n",
    "datasets_words = {\n",
    "    \"May_words\": May_data,\n",
    "    \"Clarkson_words\": Clarkson_data,\n",
    "    \"Hammond_words\": Hammond_data\n",
    "}\n",
    "\n",
    "# Apply the function to each dataset and store the results in new variables\n",
    "results_words = {}\n",
    "for name, data in datasets_words.items():\n",
    "    iterable_data = ensure_iterable(data)\n",
    "    results_words[name] = list(send_to_words(iterable_data))  # Convert generator to list\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_words = results_words[\"May_words\"]\n",
    "Clarkson_words = results_words[\"Clarkson_words\"]\n",
    "Hammond_words = results_words[\"Hammond_words\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(May_words)\n",
    "print(Clarkson_words)\n",
    "print(Hammond_words)\n",
    "\n",
    "# Check if the results are iterable\n",
    "def check_iterable(data):\n",
    "    try:\n",
    "        iter(data)\n",
    "        return True\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "print(f\"May_words is iterable: {check_iterable(May_words)}\")\n",
    "print(f\"Clarkson_words is iterable: {check_iterable(Clarkson_words)}\")\n",
    "print(f\"Hammond_words is iterable: {check_iterable(Hammond_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "# Define the stopwords list\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "# Define the function to remove stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "# Define the datasets\n",
    "datasets = {\n",
    "    \"May_data_words_nostops\": May_words,\n",
    "    \"Clarkson_data_words_nostops\": Clarkson_words,\n",
    "    \"Hammond_data_words_nostops\": Hammond_words\n",
    "}\n",
    "\n",
    "# Ensure the data is fully loaded before processing\n",
    "def ensure_iterable(data):\n",
    "    if isinstance(data, LazyCorpusLoader):\n",
    "        return list(data)\n",
    "    return data\n",
    "\n",
    "# Apply the function to each dataset and store the results in new variables\n",
    "results = {}\n",
    "for name, data in datasets.items():\n",
    "    print(f\"Processing {name}:\")\n",
    "    print(f\"Type: {type(data)}\")\n",
    "    print(f\"First 5 items: {data[:5] if isinstance(data, list) else 'Not a list'}\")\n",
    "    \n",
    "    iterable_data = ensure_iterable(data)\n",
    "    results[name] = remove_stopwords(iterable_data)\n",
    "\n",
    "# Extract the results into distinct variables\n",
    "May_data_words_nostops = results[\"May_data_words_nostops\"]\n",
    "Clarkson_data_words_nostops = results[\"Clarkson_data_words_nostops\"]\n",
    "Hammond_data_words_nostops = results[\"Hammond_data_words_nostops\"]\n",
    "\n",
    "# Print the results to verify\n",
    "print(\"May_data_words_nostops:\", May_data_words_nostops[:5])\n",
    "print(\"Clarkson_data_words_nostops:\", Clarkson_data_words_nostops[:5])\n",
    "print(\"Hammond_data_words_nostops:\", Hammond_data_words_nostops[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Lemmatize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Basic syntax for a single list.\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_May = lemmatization(May_data_words_nostops)\n",
    "lemmatized_Clarkson = lemmatization(Clarkson_data_words_nostops)\n",
    "lemmatized_Hammond = lemmatization(Hammond_data_words_nostops)\n",
    "\n",
    "# lemmatized_May\n",
    "lemmatized_Clarkson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "## Eliminate all the null strings in the lemmatized docs:\n",
    "\n",
    "def clean_texts(lemmatized_texts):\n",
    "\treturn [doc for doc in lemmatized_texts if len(doc) > 0]\n",
    "\n",
    "May_cleaned_texts = clean_texts(lemmatized_May)\n",
    "Clarkson_cleaned_texts = clean_texts(lemmatized_Clarkson)\n",
    "Hammond_cleaned_texts = clean_texts(lemmatized_Hammond)\n",
    "\n",
    "# May_cleaned_texts\n",
    "# print(Clarkson_cleaned_texts)\n",
    "# print(Hammond_cleaned_texts)\n",
    "\n",
    "Clarkson_cleaned_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the file path for the output text file\n",
    "output_file_path = './data/Clarkson_cleaned_texts.txt'\n",
    "\n",
    "# Write the cleaned texts to the text file\n",
    "with open(output_file_path, 'w') as file:\n",
    "\tfor item in Clarkson_cleaned_texts:\n",
    "\t\tfile.write(\"%s\\n\" % ' '.join(item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create the Corpora, Dictionary & LDA\n",
    "\n",
    "There are 3 data frames, so create ensure consistent and efficient treatment, create a function to pass each df through the same series of steps. These include: \n",
    "\n",
    "1. create the dictionary of terms\n",
    "2. create the corpus (count of each dictionary term)\n",
    "3. create the LDA Model\n",
    "4. run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpora_and_LDA(data_sets, names, num_topics = 5):\n",
    "    results_dict = {}\n",
    "    for data, name in zip(data_sets, names):\n",
    "        # STEP 1 - Create dictionary\n",
    "        id2word = corpora.Dictionary(data)\n",
    "        print(f\"Dictionary for {name}:\")\n",
    "        print(id2word)\n",
    "\n",
    "        # STEP 2 - Create Corpus\n",
    "        texts = data\n",
    "\n",
    "        # STEP 3 - Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        print(f\"Corpus for {name}:\")\n",
    "        print(corpus)\n",
    "\n",
    "        # STEP 4- Create LDA Model\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "        print(f\"LDA Model for {name}:\")\n",
    "        print(lda_model)\n",
    "\n",
    "        # STEP 5 - Store in dictionary\n",
    "        results_dict[name] = {\n",
    "            'dictionary': id2word,\n",
    "            'corpus': corpus,\n",
    "            'lda_model': lda_model\n",
    "        }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# RESULTS_DICTIONARY - Create a dictionary of the data sets, their respective names, and the info created from the above function. \n",
    "\n",
    "data_sets = [May_cleaned_texts, Clarkson_cleaned_texts, Hammond_cleaned_texts]\n",
    "names = ['May', 'Clarkson', 'Hammond']\n",
    "results_dict = create_corpora_and_LDA(data_sets, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the keyword in the 5 topics\n",
    "# \n",
    "import pprint\n",
    "\n",
    "for name, result in results_dict.items():\n",
    "    print(f\"Results for {name}:\")\n",
    "    pprint.pprint(result['lda_model'].print_topics())\n",
    "\n",
    "\n",
    "# print(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]\n",
    "\n",
    "#We created 5 topics. You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LDA Topic Models for each Presenter\n",
    "\n",
    "### Create a Visualize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def visualize_lda_model(results_dict, dataset_name):\n",
    "    # Access the dictionary, corpus, and LDA model for the specified dataset\n",
    "    dictionary = results_dict[dataset_name]['dictionary']\n",
    "    corpus = results_dict[dataset_name]['corpus']\n",
    "    lda_model = results_dict[dataset_name]['lda_model']\n",
    "\n",
    "    # Prepare the visualization\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='mmds', R=30)\n",
    "\n",
    "    # Display the visualization\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the LDA Visualization for a given Presenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LDA model for 'X'\n",
    "vis = visualize_lda_model(results_dict, 'Clarkson')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Evaluation - Coherence \n",
    "\n",
    "Topic coherence measures the average similarity between top words having the highest weights in a topic i.e relative distance between the top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name in results_dict.keys():\n",
    "\tlda_model = results_dict[name]['lda_model']\n",
    "\ttexts = eval(f\"lemmatized_{name}\")\n",
    "\tdictionary = results_dict[name]['dictionary']\n",
    "\t\n",
    "\tcoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "\tcoherence_lda = coherence_model_lda.get_coherence()\n",
    "\tprint(f'Coherence Score for {name}: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Model Improvement - How many topics? \n",
    "\n",
    "## Define function to Iterate Coherence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function to iterate\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "\tcoherence_values = []\n",
    "\tmodel_list = []\n",
    "\t\n",
    "\tfor name in names:\n",
    "\t\tdictionary = results_dict[name]['dictionary']\n",
    "\t\tcorpus = results_dict[name]['corpus']\n",
    "\t\ttexts = eval(f\"lemmatized_{name}\")\n",
    "\t\t\n",
    "\t\tfor num_topics in range(start, limit, step):\n",
    "\t\t\tmodel = gensim.models.LdaModel(corpus=corpus, num_topics=num_topics, random_state=100, chunksize=200, passes=10, per_word_topics=True, id2word=dictionary)\n",
    "\t\t\tmodel_list.append(model)\n",
    "\t\t\tcoherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "\t\t\tcoherence_values.append(coherencemodel.get_coherence())\n",
    "\t\n",
    "\treturn model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the function and print the results for each name in results_dict\n",
    "limit = 10\n",
    "start = 2\n",
    "step = 1\n",
    "\n",
    "for name in names:\n",
    "\tprint(f\"Results for {name}:\")\n",
    "\tmodel_list, coherence_values = compute_coherence_values(results_dict[name]['dictionary'], results_dict[name]['corpus'], eval(f\"lemmatized_{name}\"), limit, start, step)\n",
    "\tfor m, cv in zip(range(start, limit, step), coherence_values):\n",
    "\t\tprint(f\"Num Topics = {m}, Coherence Value = {cv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
